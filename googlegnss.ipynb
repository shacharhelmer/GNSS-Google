{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Smartphone Decimeter Challenge\n",
    "##### Improve high precision GNSS positioning and navigation accuracy on smartphones\n",
    "\n",
    "## Intro\n",
    "We are Nitzan Karni (208939215) and Shachar Helmer () both of us are toward our end of our computer science degree, and eager to specialize in data science. Both of us have a day to day interaction with data engineering, data analysis and even machine learning at times. In the data science world the possiblities are endless and the number of ambitious ideas that had been thrown into the air was countless, From old (19th century) picture colorization using Generative Advarsarial Networks to Radio Signal anomaly detection to natural language coding using GPT3 to translate english to code. Eventually with huge number of incomplete idea we got together and decided to decide once and for all. We browsed the web and kaggle for hours and then we encounter a fresh new challenge that had been uploaded at the same day: \"Google Smartphone Decimeter Challenge\", it was a sign from heavens! \n",
    "Due to the fact that Nitzan is working on his \"Autonomous cars and Swarm intelligence algorithms\"seminar on  in parallel and our common interest in Autonomous vehicles technologies trying to solve the navigation positioning problem by using Data Science is the right fit for us.\n",
    "\n",
    "## About The Problem\n",
    "\n",
    "We believe there is no better explenation to the problem than the original challenge description: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/overview\n",
    "<br><br>\n",
    "Have you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.\n",
    "<br><br>\n",
    "Global Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a “jumpy” experience. For many use cases the results are not fine nor stable enough to be reliable.\n",
    "<br><br>\n",
    "This competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them.\n",
    "<br><br>\n",
    "In this competition, you'll use data collected from the host team’s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions.\n",
    "<br><br>\n",
    "If successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.\n",
    "<br>\n",
    "\n",
    "##### Our no. 1 objective\n",
    "Predict phone positioning at each sample to finest granuality possible from given measurements of GNSS system and other phone instruments such as accelometer & gyro. \n",
    "\n",
    "## The Data\n",
    "Our whole dataset in the competition is comprised of several data sources.<br>\n",
    "Each collection had been measured using the following method:<br>\n",
    "A car with at least one android device starts taking it's GNSS and phone insruments measurments, drive around the city for a while and then stop the collection. <br>\n",
    "Most of the samples are from the silicon valley area around Google HQ (not surpised).<br>\n",
    "Each dataset has a train and test version except ground truth. <br>\n",
    "We need to predict this value and submit it as the results.\n",
    "GNSS raw logs containing the GNSS raw measurements and data from other sensors like gyro and accelometer.\n",
    "derived data set which is derived measurements for GNSS data.\n",
    "ground truth contains the target variable latDeg/lngDeg\n",
    "baseline which is simple GPS prediction on lat/lon based on Weighted Least Squares from derived dataset.\n",
    "\n",
    "All datasets are present both for training data as well as for the test data\n",
    "\n",
    "For further reading on the Data please refer to: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/data\n",
    "\n",
    "### Some Domain Knowledge\n",
    "Earth is surrounded by navigation satelite systems.<br>\n",
    "Overall there are 4 global navigation systems and 2 local, ones around India and most of south Asia and another japanese one that cover most of east Asia and Oceania (Pacific Ocean and Australia region).<br>\n",
    "A sample from android GNSS comprised of communication with several satelites.<br>\n",
    "For each satelite we are measuring several metrics here is an example of several important ones:<br>\n",
    "`rawPrM` - Raw pseudorange in meters. It is the product between the speed of light and the time difference from the signal transmission time (receivedSvTimeInGpsNanos) to the signal arrival time (Raw::TimeNanos - Raw::FullBiasNanos - Raw::BiasNanos).<br>\n",
    "`[x/y/z]SatPosM` - The satellite position (meters) in an ECEF coordinate frame at best estimate of “true signal transmission time” defined as ttx = receivedSvTimeInGpsNanos - satClkBiasNanos (defined below). They are computed with the satellite broadcast ephemeris, and have ~1-meter error with respect to the true satellite position.<br>\n",
    "`[x/y/z]SatVelMps` - The satellite velocity (meters per second) in an ECEF coordinate frame at the signal transmission time (receivedSvTimeInGpsNanos). They are computed with the satellite broadcast ephemeris, with this algorithm.\n",
    "<br><br>\n",
    "Each sample also contains satelite atomic clock metrics and several parameters that can affect the delay and noise of the measurements such as clock drift, ionospheric layer delay, tropospheric layer delay (Layers of earth's sky that can cause measurement deviation, similar to the well known Atmosphere layer).<br><br>\n",
    "Essentially the phone will try and communicated with as many satelites it can reach from its position. The more the merrier.<br>In our final calculation the more data we can interpolate regarding the positioning of the phone the finer our result would get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import gmplot\n",
    "from IPython.display import IFrame\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import simdkalman\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "from functools import reduce\n",
    "import math\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate through the folder structure to setup our derived & ground truth dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_files_to_df(path_list):\n",
    "    \"\"\"\n",
    "    Read a list of structured files in csv format, concatenating them into a single DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_list: list of paths to csv datasets\n",
    "    \"\"\"\n",
    "    return reduce(lambda df1, df2: pd.concat([df1, df2]), [pd.read_csv(s) for s in path_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simplify_set_file_name(file_name):\n",
    "    \"\"\"\n",
    "    Convert the collection name to human readble format according to:\n",
    "    [train]/[drive_id]/[phone_name]/\n",
    "    \"\"\"\n",
    "    path_rest = file_name.split('google-smartphone-decimeter-challenge/')[1]\n",
    "    trn_grnd = path_rest.split('/')[0]\n",
    "    path_rest = path_rest.replace(f'{trn_grnd}/', '')\n",
    "    date = path_rest.split('/')[0]\n",
    "    path_rest = path_rest.replace(f'{date}/', '')\n",
    "    phone = path_rest.split('/')[0]\n",
    "    return f'{trn_grnd} | {date} | {phone}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_data_dir = 'google-smartphone-decimeter-challenge'\n",
    "\n",
    "train_collections = [f'{base_data_dir}/train/{collection}' for collection in os.listdir(f'{base_data_dir}/train')]\n",
    "test_collections = [f'{base_data_dir}/test/{collection}' for collection in os.listdir(f'{base_data_dir}/test')]\n",
    "\n",
    "print(f'{len(train_collections)} train collections, {len(test_collections)} test collections\\n')\n",
    "\n",
    "derived_train_sets = [f'{c}/{s}/{s}_derived.csv' for c in train_collections for s in os.listdir(c)]\n",
    "ground_train_sets = [f'{c}/{s}/ground_truth.csv' for c in train_collections for s in os.listdir(c)]\n",
    "derived_test_sets = [f'{c}/{s}/{s}_derived.csv' for c in test_collections for s in os.listdir(c)]\n",
    "\n",
    "\n",
    "drvd_trn_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in derived_train_sets])\n",
    "grnd_trn_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in ground_train_sets])\n",
    "drvd_tst_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in derived_test_sets])\n",
    "print('Do all columns appear in all the data sets?')\n",
    "print(f'Derived train data: {drvd_trn_clms == set(list(pd.read_csv(derived_train_sets[0], nrows=1).columns))}')\n",
    "print(f'Ground truth train data: {grnd_trn_clms == set(list(pd.read_csv(ground_train_sets[0], nrows=1).columns))}')\n",
    "print(f'Derived test data: {drvd_tst_clms == set(list(pd.read_csv(derived_test_sets[0], nrows=1).columns))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived data set contains samples uniquely defined by:\n",
    "- The device type (phoneName)\n",
    "- The collection it is (at which session was it sampled)\n",
    "- the time the sample was taken at in milliseconds passed since the GPS epoch.\n",
    "\n",
    "Each such unique GPS sample has a varying amount of satelite samples describing it (rows in the derived data set), yet they all refer to the same time-location combination.</br>\n",
    "In the ground truth data set, samples are matched with rows in an \"on\" and \"one-to-one\" fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived = read_files_to_df(derived_train_sets)\n",
    "derived.drop('index', axis=1, inplace=True)\n",
    "ground = read_files_to_df(ground_train_sets)\n",
    "print(\"# of samples in ground truth train dataset: {}\\n# of samples in derived train dataset: {}\\n\".format(drvd_trn.shape[0], grnd_trn.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bsln_trn = pd.read_csv('google-smartphone-decimeter-challenge/baseline_locations_train.csv')\n",
    "bsln_tst = pd.read_csv('google-smartphone-decimeter-challenge/baseline_locations_test.csv')\n",
    "target = ground.merge(bsln_trn, how='inner', on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], suffixes=('_grnd', '_bsln'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "### EDA on derived dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvd_idx = derived.set_index(['collectionName','phoneName', 'millisSinceGpsEpoch'])\n",
    "print(\"The number of different satelites is: {}\".format(derived['svid'].nunique()))\n",
    "g = derived.groupby(['collectionName','phoneName', 'millisSinceGpsEpoch']).agg({'svid': [ len,lambda x: np.bincount(x).argmax()]})\n",
    "g.rename({'<lambda_0>': 'frequency', 'len': 'numOfSamples'}, axis=1, inplace=True)\n",
    "bnc = np.bincount(g[('svid', 'frequency')])\n",
    "print(\"The most frequent satelite is: {} with {} occurrences\".format(bnc.argmax(), max(bnc)))\n",
    "g.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed a single sample in a collection can be made out of as many as 109 datapoints. \n",
    "There are only 37 satelites, so we need to find out the source for the multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = g[g['svid', 'numOfSamples']==g['svid','numOfSamples'].max()]\n",
    "s2 = drvd_idx[drvd_idx['svid']==s.iloc[0]['svid','frequency']].loc[s.index[0]]\n",
    "s2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From index column it is clear that we are currently inspecting 12 datapoints from single satelite at specific sample.<br>\n",
    "The unique table doesn't give out an immidiate suspect with 12 unique values, it is probably a combination of columns. <br>\n",
    "Lets try constellationType + signalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2['constellationSignal'] = s2['constellationType'].astype(str) + '_'+ s2['signalType']\n",
    "s2['constellationSignal'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 combinations for this sample therefore the  constellationType + signalType combination is not to blame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2.groupby(s2.drop('index', axis=1).columns.tolist(),as_index=False).size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple datapoints with duplicate rows, but still there are 8 different rows after grouping by all columns to find the pure duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2.groupby(['rawPrM', 'receivedSvTimeInGpsNanos', 'constellationSignal'],as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements are recieved at different timestamps and therefore the raw pseodo range is affected because of the movement of the satelite.<br>\n",
    "After dropping duplicates we still going to remain with several measurements per svid on specific sample due to multiple polling at different recieved times.<br>\n",
    "Before handeling the data we should drop the duplicate rows containing exactly the same values at every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "old_derived_count = derived.shape[0]\n",
    "derived.drop_duplicates(inplace=True)\n",
    "print(\"Dropped {} duplicate measurements\".format(old_derived_count-derived.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Values</h2>\n",
    "<h3>Distribution</h3><br>\n",
    "Lets have a look at how the number samples are distributed over the ground truth dataset by the collection.<br>\n",
    "Then we will try and compare it to the derived data set distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,15))\n",
    "ax = fig.gca()\n",
    "sns.countplot(data=ground, y='collectionName', hue='phoneName', ax=ax, palette='rocket')\n",
    "ax.legend(loc='upper right')\n",
    "ax.yaxis.grid(True)\n",
    "plt.title('Ground truth number of samples per collection')\n",
    "fig = plt.figure(figsize=(8,15))\n",
    "ax = fig.gca()\n",
    "sns.countplot(data=derived, y='collectionName', hue='phoneName', ax=ax, palette='rocket')\n",
    "ax.legend(loc='upper right')\n",
    "ax.yaxis.grid(True)\n",
    "plt.title('Derived number of samples per collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mobile device type distribution over test and train collections is identical as is aserted and displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pieplot_on_columns(data, columns):\n",
    "    for c in columns:\n",
    "        data_dist = data.groupby(c).size().to_frame('size')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.title.set_text(f'{c} distribution by # of samples')\n",
    "        ax.pie(data_dist['size'], labels=list(data_dist.index), autopct=lambda x: f'{int(x)}%')\n",
    "        plt.show()\n",
    "\n",
    "pieplot_on_columns(derived, ['phoneName', 'signalType', 'constellationType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we are displaying a heatmap of the number of samples per satelite for each collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = derived[['collectionName', 'svid', 'millisSinceGpsEpoch']].groupby(['collectionName', 'svid']).count()\n",
    "d.reset_index(inplace=True)\n",
    "d = d.pivot_table(columns='svid', values='millisSinceGpsEpoch', index='collectionName', fill_value=0)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.gca()\n",
    "sns.heatmap(data=d, ax=ax)\n",
    "plt.title(\"Collection to satelite number of samples heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot constellation types\n",
    "# plot signal types\n",
    "# understand if all derived samples (key samples contain each of which signals and constellations)\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(8, 8, forward=True)\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['xSatPosM', 'ySatPosM', 'zSatPosM']]), ax = axs[0][0])\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['xSatVelMps', 'ySatVelMps', 'zSatVelMps']]), ax = axs[0][1])\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['ionoDelayM', 'tropoDelayM']]), ax = axs[1][0])\n",
    "sns.boxplot(data=pd.melt(derived[['correctedPrM', 'rawPrM']]), x='variable',  y='value', ax= axs[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the evaluation metric\n",
    "The challenge evaluation metric is set to be as following:<br>\n",
    "https://www.kaggle.com/c/google-smartphone-decimeter-challenge/overview/evaluation<br>\n",
    "Submissions are scored on the mean of the 50th and 95th percentile distance errors. For every phone and at every millisSinceGpsEpoch,<br> the horizontal distance (in meters) is computed between the predicted lat/lng and the ground truth lat/lng.<br> These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller).<br> The 50th and 95th percentile errors are then averaged for each phone.<br> Lastly, the mean of these averaged values is calculated across all phones in the test set.<br><br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "The harversine function determines the \"Great circle\" distance between 2 latlon datapoints.<br>\n",
    "We are using this measurement to accurratly determine as needed the `horizontal distance` between two points on earth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition metric that is required is calculating the distances between the true and predicted values.<br>\n",
    "Then return the mean of the median and 95th percentile out of that distance vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def haversine_50thP_95thP_mean(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Using calc_haversine formula to calculate the mean of  the mean of the 50th and 95th percentile distance errors\n",
    "    The Competition evaluation metric\n",
    "    \"\"\"\n",
    "    haversine = calc_haversine(lat1, lon1, lat2, lon2)\n",
    "    return (np.percentile(haversine, 95) + np.median(haversine)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are set to go with already predicted dataset of samples.<br>\n",
    "What is so bad with the initial predictions that we even need to make an effort to minimize its error?<br>\n",
    "Let's have a look at the competition evaluation for the given predictions and try to asses how they differ from the ground truth.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_prev = haversine_50thP_95thP_mean(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln'], target['lngDeg_bsln'])\n",
    "print(\"{}m is the mean of the meadian error and the 95th percentile\".format(score_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As implied in the compition name 6.2 meters is simply not enough.<br>\n",
    "we have to reduce the prediction error to at least under 1m to get into the decimeter realm.<br>\n",
    "6 meters doesn't seem to be that far but soon we would see it is not an easy task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the haversine calculation we can compare the baseline estimated coordinates to the ground truth, sliced by various columns. Thus we might get an intuition for more and less \"accurate\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drvd_grnd_bsln['hvr_dist'] = calc_haversine(drvd_grnd_bsln.latDeg, drvd_grnd_bsln.lngDeg, drvd_grnd_bsln.latDeg_bsln, drvd_grnd_bsln.lngDeg_bsln)\n",
    "\n",
    "def evaluate_from_dist(hvr_dist):\n",
    "    return (np.nanpercentile(hvr_dist, 95) + np.nanmedian(hvr_dist)) / 2\n",
    "\n",
    "cltn_hvr_dist = drvd_grnd_bsln.groupby('collectionName')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "phone_hvr_dist = drvd_grnd_bsln.groupby('phoneName')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "sig_type_hvr_dist = drvd_grnd_bsln.groupby('signalType')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "svid_hvr_dist = drvd_grnd_bsln.groupby('svid')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(18, 11, forward=True)\n",
    "axs[0][0].set_title('collections compared with the haversine distance their samples produce')\n",
    "sns.barplot(x='hvr_dist', y='collectionName', data=cltn_hvr_dist, ax=axs[0][0])\n",
    "axs[0][1].title.set_text('Phone names compared with the haversine distance their samples produce')\n",
    "sns.barplot(x='hvr_dist', y='phoneName', data=phone_hvr_dist, ax=axs[0][1])\n",
    "axs[1][0].title.set_text('Signal types compared with the haversine distance that samples of that type produce')\n",
    "sns.barplot(x='hvr_dist', y='signalType', data=sig_type_hvr_dist, ax=axs[1][0])\n",
    "axs[1][1].title.set_text('Satelite ID compared with the haversine distance that its samples produce')\n",
    "sns.barplot(y='hvr_dist', x='svid', data=svid_hvr_dist, ax=axs[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An interesting outcome! From this plot we can immidiatly learn about two very problematic collections.<br>\n",
    "`2021-04-22-US-SJC-1` and `2021-04-29-US-SJC-2`, San Jose seems as a city which the measurements are hard to predict. <br>\n",
    "- Another interesting finding is the error score of `SamsungS20Ultra` which is by far much more worst than the other phones.<br>\n",
    "- Phones using BeiDu (Chinese) signalType are experiencing slightly worst GNSS performance<br>\n",
    "- Satelites 34 & 37 are noticably worst performers than the rest of the bunch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline vs. ground truth spatial comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to analyze how the baseline predictions are positioned in comparison to the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_distance'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln'], target['lngDeg_bsln'])\n",
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the error is quite large using simply the baseline data (Weighted Least Squars on derived data).<br>\n",
    "Further more we have some very strange outlier measurements that with very large errors that are unacceptable at any circumstances and do not even imply on simple GPS accurracy.<br> These outlier samples can range all the way from 40m error to 8km error.<br> In the future we would use outlier detection algorithm to detect these samples and then clean them.<br>\n",
    "\n",
    "Our mission is to flat that plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = (target['bsln_grnd_distance']<0.5).apply(lambda x: 'g' if x else 'r')\n",
    "size = (target['bsln_grnd_distance']<0.5).apply(lambda x: 2 if x else 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121); plt.title('Baseline vs. ground truth latDeg'); \n",
    "plt.scatter(target['latDeg_grnd'], target['latDeg_bsln'], color=colors, s=size)\n",
    "plt.xlabel('ground truth latDeg'); plt.ylabel('baseline latDeg')\n",
    "plt.subplot(122); plt.title('Baseline vs. ground truth lngDeg');\n",
    "plt.scatter(target['lngDeg_grnd'], target['lngDeg_bsln'], color=colors, s=size)\n",
    "plt.xlabel('ground truth lngDeg'); plt.ylabel('baseline lngDeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we can see two plots. One for latDeg and the other for lngDeg.<br>\n",
    "Although it seems like a line it is a scatter plot, each point would be colored in green if the horizontal distance<br>\n",
    "from baseline prediction is lower than 5 decimeters otherwise in red. our ambision is to make transform the plot to green line that corripond to the y=x line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if np.all(bsln_trn.reset_index()['millisSinceGpsEpoch'] == ground.reset_index()['millisSinceGpsEpoch']):\n",
    "    print(\"Baseline and ground truth data are synchronized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot collection route on a map using ground truth route vs. baseline route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compareRoutes(true_route, predicted_route, mapName='map'):\n",
    "    \"\"\"Recieve as input two routes one as the ground truth and the other as the predicted.\n",
    "    The ground truth would be drawn as line on the map.\n",
    "    The predicted would be drawn as heatmap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_route : numpy 2d array\n",
    "        The true route the sample has went through\n",
    "    predicted_route : numpy 2d array\n",
    "        The predicted route\n",
    "    mapName : str\n",
    "        defualt = 'map'\n",
    "        The name of the generated html google map\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    mapName.html file containing the google map.\n",
    "    \"\"\"\n",
    "    gmap1 = gmplot.GoogleMapPlotter(true_route[true_route.shape[0]//2,0], true_route[true_route.shape[0]//2,1], 14, apikey='AIzaSyB0ONxmQBgtM14DqTRDrYBBUw2-woWkCIE', map_type='hybrid')\n",
    "    gmap1.plot(true_route[:,0],true_route[:,1],'cornflowerblue', edge_width=2)\n",
    "    gmap1.heatmap(predicted_route[:,0],predicted_route[:,1])\n",
    "    gmap1.draw( \"{}.html\".format(mapName) )\n",
    "    # Print the map to notebook\n",
    "    return IFrame(src=\"./{}.html\".format(mapName), width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following map we can see the baseline predicted values vary around the ground truth<br>\n",
    "The route in blue represent the true route of the measurements while the hit map represent the baseline measurement.<br>\n",
    "The hotter the color gets, the more frequent and concentraited the predictions are.<br>\n",
    "You can interact with the map as with any google map. <br>\n",
    "Try to zoom in and have a look at the prediction distribution around the route and how it diviates from the ground truth<br>\n",
    "We can also notice why decimeter prediction is neccessary as jumpy navigation system can cause quite distress,<br>\n",
    "Especially if we finding ourself outside the road or even on the roof of some building as we can observe in multiple observations.<br>\n",
    "High accurracy navigation systems such as ADAS, Autonomous vehicles or even Pokemon GO have critical neccessity in highly accurrate lat lng positioning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drawRandomPath():\n",
    "    t = target[target['collectionName'] == target['collectionName'].sample().iloc[0]]\n",
    "    return compareRoutes(np.array(t[['latDeg_grnd','lngDeg_grnd']]), np.array(t[['latDeg_bsln','lngDeg_bsln']]))\n",
    "drawRandomPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotSatelite(ax, x, phone_cor=None):\n",
    "    \"\"\"Plot satelite measurements for a specific sample.\n",
    "    \"\"\"\n",
    "    ax.scatter(x['xSatPosM'], x['ySatPosM'], x['zSatPosM'], c='r',s=50)\n",
    "    ax.quiver(x['xSatPosM'], x['ySatPosM'], x['zSatPosM'], x['xSatVelMps'], x['ySatVelMps'], x['zSatVelMps'], length=(x['xSatVelMps']**2+x['ySatVelMps']**2+x['zSatVelMps']**2)**0.5)\n",
    "    if isinstance(x.name, (int, np.integer)):\n",
    "        ax.text(x['xSatPosM'], x['ySatPosM'], x['zSatPosM']+10, str(x.name))\n",
    "    if phone_cor is not None:\n",
    "        ax.plot([x['xSatPosM'], phone_cor[0]], [x['ySatPosM'], phone_cor[1]], [x['zSatPosM'], phone_cor[2]], c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotSphere(ax, r=6731000, center=(0,0,0), hRange=(0, 2 * np.pi), vRange=(0, np.pi), phoneLat=0, phoneLng=0):\n",
    "    theta = np.array([np.linspace(hRange[0], hRange[1], 50)])\n",
    "    theta = np.ones_like(theta).T @ theta\n",
    "    phi = np.array([np.linspace(vRange[0], vRange[1], 50)])\n",
    "    phi = np.ones_like(phi).T @ phi\n",
    "    phi = phi.T\n",
    "    \n",
    "    xx = r * np.sin(phi) * np.cos(theta) + center[0]\n",
    "    yy = r * np.sin(phi) * np.sin(theta) + center[1]\n",
    "    zz = r * np.cos(phi) + center[2]\n",
    "    \n",
    "    ax.plot_surface(xx, yy,zz)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def latlonToXYZ(lat, lon):\n",
    "    r=6731000 # Earth radius\n",
    "    return (float(r * np.sin(lat) * np.cos(lon)), float(r * np.sin(lat) * np.sin(lon)), float(r * np.cos(lat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospacial data analysis on derived dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single sample 3d visualization\n",
    "for single android measurment visualize the specific point on earth the phone exists and the connected satelites,\n",
    "Exibit the velocity of every satelite and the distance from the phone measured as the corrected pseudo range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "derived.reset_index(inplace=True)\n",
    "ground.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index of every sample is attributed to the collection, the phone used in the collection set <br>(The data is collected using multiple android phones in driving car)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "derived['millisSinceGpsEpoch_drvd'] = derived['millisSinceGpsEpoch']\n",
    "derived_idx = derived.set_index(['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth vs. derived `millisSinceGpsEpoch` comparison\n",
    "* The baseline and derived are correlated. moreover baseline is genrated using derived dataset\n",
    "* It seems that unfortunatly there is a difference between the timestamp of the samples in the ground truth\n",
    "  and the baseline as shown in the following. there are some samples that are the same regarding the timestamp and others which not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if np.all(bsln_trn.reset_index()['millisSinceGpsEpoch'] == ground['millisSinceGpsEpoch']):\n",
    "    print(\"Baseline and ground truth data are synchronized\")\n",
    "if derived['millisSinceGpsEpoch'].nunique() == ground['millisSinceGpsEpoch'].nunique():\n",
    "    print(\"Derived and ground truth data has the same number of time ephocs\")\n",
    "    if np.all(derived['millisSinceGpsEpoch'] == ground['millisSinceGpsEpoch']):\n",
    "        print(\"Derived and ground truth data are synchronized\")\n",
    "    else:\n",
    "        print(\"Derived and ground truth data are not synchronized\")\n",
    "else:\n",
    "    print(\"Derived and ground truth data has different number of time ephocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose random sample from derived data to display satelites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot tries to sums up a single measurement in the derived dataset.<br>\n",
    "The blue sphere represent the earth.<br>\n",
    "The pink dot represents the north pole.<br>\n",
    "The green dot represents the android device making the measurement.<br>\n",
    "Each red dot represents a satelite in space.<br>\n",
    "Each satelite has vector representing its speed.<br>\n",
    "Each satelite streches a line in green which represents the distance to the the android device.<br>\n",
    "All the locations are appearing in ECEF coordinated system, with earch fixed in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = derived[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].sample()\n",
    "while not np.any(ground['millisSinceGpsEpoch']==sample.values[0,2]):\n",
    "    sample = derived[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].sample()\n",
    "sample_target = ground[ground['millisSinceGpsEpoch']==sample.values[0,2]]\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "#sample = derived_orig[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].sample()\n",
    "satelites = derived_idx.loc[sample.values[0,0], sample.values[0,1], sample.values[0,2]]\n",
    "phone_cor = latlonToXYZ(sample_target['latDeg'], sample_target['lngDeg'])\n",
    "north_pole = latlonToXYZ(90, 0)\n",
    "satelites.apply(lambda sat: plotSatelite(ax, sat, phone_cor), axis=1)\n",
    "plt.title(\"collectionName: {}  || Phone: {} ||  timestamp: {} ms\".format(sample.values[0,0], sample.values[0,1], sample.values[0,2]))\n",
    "ax.scatter(phone_cor[0],phone_cor[1],phone_cor[2], c='g', s=100)\n",
    "ax.scatter(north_pole[0],north_pole[1],north_pole[2], c='pink', s=100)\n",
    "ax.text(phone_cor[0]+1000,phone_cor[1]+1000,phone_cor[2]+1000, \"lat:{}, lng:{}\".format(float(sample_target['latDeg'],), float(sample_target['lngDeg'])))\n",
    "plotSphere(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman filter\n",
    "The kalman filter is an method to smooth estimated measurements using it's noice.<br>\n",
    "The filter works iterativly on timeseries data, at each epoch the `Kalman Gain` is calculated `KG = ERR_EST / (ERR_EST + ERR_MEA)`,<br> whereas `ERR_EST` represent the error in the estimation and `ERR_MEA` represent the error in the measurement.<br>\n",
    "Then the new estimated position is calculated using The following equation `EST_t-1 + KG(EST_t - EST_t-1)` <br>\n",
    "then the new error in the estimation is calculated using `(1-KG)ERR_MEA`.<br>\n",
    "This set of iterativly equations will determine the weight to give to each estimation over time and will smooth out the estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phone_col = 'phonePath'\n",
    "bsln_trn[phone_col] = bsln_trn['collectionName'] + bsln_trn['phoneName']\n",
    "lat_col = 'latDeg'\n",
    "lon_col = 'lngDeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kf):\n",
    "    unique_paths = df[phone_col].unique()\n",
    "    for phone in tqdm(unique_paths):\n",
    "        data = df.loc[df[phone_col] == phone][[lat_col, lon_col]].values\n",
    "        data = data.reshape(1, len(data), 2)\n",
    "        smoothed = kf_.smooth(data)\n",
    "        df.loc[df[phone_col] == phone, lat_col] = smoothed.states.mean[0, :, 0]\n",
    "        df.loc[df[phone_col] == phone, lon_col] = smoothed.states.mean[0, :, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn_orig = bsln_trn.copy()\n",
    "bsln_trn_sm = apply_kf_smoothing(bsln_trn)\n",
    "target['bsln_grnd_kalman'] = calc_haversine(ground['latDeg'], ground['lngDeg'], bsln_trn['latDeg'], bsln_trn['lngDeg'])\n",
    "target[['bsln_grnd_kalman', 'bsln_grnd_distance']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things we notice right away is that the standard diviation decreased significantly by half.<br>\n",
    "Meaning our data is now much less noisy and jumpy, <br>\n",
    "The maximum value had reduced it's error to by more than 70% while the minimum value had been raised quite sharply by 600%<br>\n",
    "All the other metrics had been benifited from the kalman smoothing which decreased the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'], label='raw estimates', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman'], label='kalman smoothed', alpha=0.7); \n",
    "plt.title('Smoothed baseline estimates using kalman filter'); \n",
    "plt.xlabel('no. sample'); \n",
    "plt.ylabel('horizontal distance (m)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some of our highst peaks in the error distance had been lowered.<br>\n",
    "Overall nice smooth action which flatted out some jittery predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_curr = haversine_50thP_95thP_mean(ground['latDeg'], ground['lngDeg'], bsln_trn['latDeg'], bsln_trn['lngDeg'])\n",
    "print(\"{}m error rate from our evaluation metric.\\n an impovement of {}m\".format(score_curr, score_prev-score_curr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement of 0.77m simply by using kalman filter with basic hyperparameters.<br>\n",
    "In the future we would like to tune the kalman filter hyperparameters for better results and even incorprate measurements from other devices like accelometer and gyro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean latDeg & lngDeg in baseline dataset over phones at the same epoch\n",
    "Each collection is made by driving car using one or more androind deviced to collect GNSS logs.<br>\n",
    "At each epoch several devices can collect GNSS data. Each of them should have the same lat/lng position<br>\n",
    "Therefore we would average over the devices the lat lng degrees.<br>\n",
    "As we can see not all the phones in our collections are synchronyzed.<br>\n",
    "Therefore we would like to mean the bucket of quarter of the seconds of epochs to leave some space for unsynchronized phones to average themeself.<br>\n",
    "The choice of 250 milliseconds was made by trial and error.<br>\n",
    "We using averaging the prediction whereas an assumption on the speed between each 250ms bucket is at max 65 mph (California highway speed limit) which between each bucket leave room for 3.6m error after averaging. most of the driving in the dataset is <br>made within the city, therefore an presumably the average speed is much lower so the error is much less critical.<br> Further invistagation regarding the speed at which the samples where taken will be introduced later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn['qsSinceGpsEpoch'] = bsln_trn.millisSinceGpsEpoch//250\n",
    "bsln_trn['millisSinceFirstEpoch'] = bsln_trn.millisSinceGpsEpoch - min(bsln_trn.millisSinceGpsEpoch)\n",
    "\n",
    "df = bsln_trn.groupby(['collectionName', 'qsSinceGpsEpoch']).agg({'latDeg': [ np.mean ], 'lngDeg': [ np.mean ], 'phoneName': [list], 'millisSinceFirstEpoch': [list]})\n",
    "bsln_mean_smoothed = pd.merge(bsln_trn, df.reset_index(), how='left', on=['collectionName', 'qsSinceGpsEpoch'], suffixes=('raw', 'mean'))\n",
    "score_prev = score_curr\n",
    "score_curr = haversine_50thP_95thP_mean(target['latDeg_grnd'], target['lngDeg_grnd'], bsln_mean_smoothed[('latDeg','mean')], bsln_mean_smoothed[('lngDeg','mean')])\n",
    "print(\"{}m error rate from our evaluation metric.\\n an impovement of {}m\".format(score_curr, score_prev-score_curr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement of 2.7 decimeters, not much but still not bad for simply avrage the different phones measurements over one sample.<br> It gets us to an error of 5.21m even before applying any ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_kalman_mean_smooth'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], bsln_mean_smoothed[('latDeg','mean')], bsln_mean_smoothed[('lngDeg','mean')])\n",
    "target[['bsln_grnd_kalman_mean_smooth', 'bsln_grnd_kalman', 'bsln_grnd_distance']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics look a bit better for our smoothed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'], label='raw estimates', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman'], label='kalman smoothed', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman_mean_smooth'], label='kalman + mean smoothed', alpha=0.7);\n",
    "plt.title('Smoothed baseline estimates using kalman filter + mean phone average'); \n",
    "plt.xlabel('no. sample'); \n",
    "plt.ylabel('horizontal distance (m)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some smaller peaks had been smoothed. some had been lowered down, while some more accurate measurements had been pulled upwards<br>\n",
    "Overall it seems much better than the raw baseline measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn = bsln_mean_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target['latDeg_bsln'] = bsln_trn['latDeg']\n",
    "target['lngDeg_bsln'] = bsln_trn['lngDeg']\n",
    "drawRandomPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & data preperation\n",
    "\n",
    "* Clean the data from unwanted values\n",
    "* Generate added value features that can benefit our module.\n",
    "* Prepare the data for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate corrected pseudo range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite) can be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "derived['correctedPrM'] = derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - derived['ionoDelayM'] -derived['tropoDelayM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Foreach sample take the previous location of the phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As timeseries nature of the collection it is expected to give our previous estimation weight in our current estimation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bsln_trn['latDeg_prv'] = bsln_trn['latDeg']\n",
    "bsln_trn['lngDeg_prv'] = bsln_trn['lngDeg']\n",
    "bsln_trn.loc[bsln_trn['collectionName']==bsln_trn.shift()['collectionName'], 'latDeg_prv'] = bsln_trn.shift()['latDeg']\n",
    "bsln_trn.loc[bsln_trn['collectionName']==bsln_trn.shift()['collectionName'], 'latDeg_prv'] = bsln_trn.shift()['lngDeg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add datetime timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of analysis create timestamp columns which are much more human readable than `millisSinceGpsEpoch`<br>\n",
    "note - `millisSinceGpsEpoch` is the milliseconds passed since 6th Jan 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseTime = datetime.datetime(1980,1,6,0,0,0,0)\n",
    "derived['epoch_timestamp'] = derived['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))\n",
    "ground['epoch_timestamp'] = ground['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))\n",
    "bsln_trn['epoch_timestamp'] = bsln_trn['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of the derived dataset we will narrow it down to collection-phone-epoch samples which each samples contains the relative <br>\n",
    "information regarding each satelite in the sample. If the satelite is not appearing in the current measurement all it's fields would <br>\n",
    "There are usually more than 1 sample per specific `collectionName`-`phoneName`-`millisSinceGpsEpoch`-`svid` therefore we will<br> groupby this key and use mean on our numric features before using pivot table.<br>\n",
    "correspondedly be equal to NaN. We cannot simply use 0 as our distance from them and wrap it up with `fillna(0)` <br>\n",
    "We would have to find a way to make our model ignore these values, because there is no such thing as minus distance from earth <br>\n",
    "We will assign NaN distances with `-max(correctedPrM)` to make sure our module is encourged to diffrentiate it from the true measurements and ignore it<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = {'correctedPrM', 'xSatPosM', 'ySatPosM', 'zSatPosM', 'signalType', 'xSatVelMps', 'ySatVelMps', 'zSatVelMps', 'satClkBiasM', 'satClkDriftMps'  }\n",
    "df = derived[list(features.union({'collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid'}))].groupby(['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid']).mean()\n",
    "piv = df.pivot_table(values=list(features), index=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], columns=['svid'])\n",
    "grouped = derived[['phoneName','collectionName', 'millisSinceGpsEpoch', 'receivedSvTimeInGpsNanos', 'epoch_timestamp']].groupby(['phoneName', 'millisSinceGpsEpoch']).max()\n",
    "piv = piv.merge(grouped, on=['phoneName', 'millisSinceGpsEpoch'])\n",
    "piv.fillna(-max(derived['correctedPrM']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and correlate the derived data to the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seen before, the number of derived epochs is different that the number of epoch is ground truth dataset. <br>\n",
    "This is a huge problem because we need to correlate the target data to the training data. <br>\n",
    "Our main objective is to predict accurratly the lat/lng position of the phone in the world using the derived data and baseline data.\n",
    "<br>\n",
    "One obsticle in doing so is that the derived dataset grouped by time epochs does not necesseraly own the same time epochs as baseline.<br>\n",
    "This problem reoccurres again when trying to compare to the target data.<br>\n",
    "So before we even do any training we need to figure out the cause for this incosistency. Or at least when does it occurres and at which scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn['epoch_timestamp'] = bsln_trn['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bsln = bsln_trn.set_index('millisSinceGpsEpoch').sort_index()\n",
    "piv = piv.reset_index().set_index('millisSinceGpsEpoch').sort_index()\n",
    "df = pd.merge_asof(bsln, piv, on='millisSinceGpsEpoch',by='phoneName', suffixes=('_bsln', '_piv'), direction='nearest', tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"No. of rows that are outside the tolerance range are {}.\\nThere are total {} samples\".format(df['epoch_timestamp_piv'].isna().sum(), df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bsln_trn.shape[0]-piv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"{}% of the baseline is missing from derived\".format((df['epoch_timestamp_piv'].isna().sum()/df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed before the derived data has 1003 missing records from baseline.<br>\n",
    "Therefore we can be satisfied with 1013 missing which is a very small percentage of the data.<br>\n",
    "nethertheless it is still part of the data and we should decide what to do next with our null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(df, ground[['phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], how='inner', left_on=['phoneName', 'millisSinceGpsEpoch'], right_on=['phoneName','millisSinceGpsEpoch'], suffixes=('_bsln', '_grnd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.drop(['latDeg_bsln_prev', 'lngDeg_bsln_prev'], axis=1, inplace=True)\n",
    "target = data[['latDeg_grnd', 'lngDeg_grnd']]\n",
    "\n",
    "bsln_train = bsln_trn[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'phone', 'latDeg', 'lngDeg','latDeg_prv', 'lngDeg_prv', 'heightAboveWgs84EllipsoidM']]\n",
    "bsln_train = bsln_train.merge(ground[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']],\n",
    "                 on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner',  suffixes=('_bsln', '_grnd')) #precision use in samples without data / labels\n",
    "bsln_train = pd.get_dummies(bsln_train, columns=['phoneName'])\n",
    "train = bsln_train.drop(['latDeg_grnd', 'lngDeg_grnd', 'collectionName', 'millisSinceGpsEpoch', 'phone'], axis=1)\n",
    "labels = bsln_train[['latDeg_grnd', 'lngDeg_grnd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the error correction nature of the problem, initially we would like to see how a ML model can reproduce the simple baseline prediction<br>\n",
    "using only its baseline latlon and even try and make it better by using timeseries model such as LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = train[['latDeg_bsln', 'lngDeg_bsln']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for train/test the data & normilize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the model to perform better on the data we will normalize our data before feeding it to the model.<br>\n",
    "We chose MinMaxScaler which perform the following `x_i = (x_i - min(X))/(max(X)-min(X))`, the choice has been made <br>\n",
    "due to the need to preserve the order between the lat lon locations.\n",
    "\n",
    "Then we split the normalized dataset to train and validation sets by 80%/20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "n_features = X.shape[1]\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_normed = scaler_x.fit_transform(X)\n",
    "y_normed = scaler_y.fit_transform(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_normed, y_normed, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function\n",
    "Our NN model would be implemented using `tensorflow` library.<br>\n",
    "Because the evaluation metric of the competition is the haversine distance loss, <br>\n",
    "A good NN implementation would reduce this following metric.<br>\n",
    "Therefore we will build a loss function exactly for that.<br>\n",
    "`Keras` library make sure to take into account the gradients calculations using the given loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def haversine_loss(y_true, y_pred):\n",
    "    PI_ON_180 = tf.constant(np.pi / 180, dtype=tf.float32)\n",
    "    RADIUS_M = tf.constant(6_377_000, dtype = tf.float32)\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "\n",
    "    yt_rad = y_true * PI_ON_180\n",
    "    yp_rad = y_pred * PI_ON_180\n",
    "\n",
    "    delta = yt_rad - yp_rad\n",
    "    v = delta / 2\n",
    "    v = tf.sin(v)\n",
    "    v = v**2\n",
    "\n",
    "    a = v[:,1] + tf.cos(yt_rad[:,1]) * tf.cos(yp_rad[:,1]) * v[:,0] \n",
    "    c = tf.sqrt(a)\n",
    "    c = 2* tf.math.asin(c)\n",
    "    c = c*RADIUS_M\n",
    "    \n",
    "    p50 = tfp.stats.percentile(c, 50)\n",
    "    p95 = tfp.stats.percentile(c, 95)\n",
    "    \n",
    "    final = tf.reduce_mean(tf.convert_to_tensor([p50, p95]))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple NN model with only one hidden layer. <br>\n",
    "The use of `tanh` activation function is made due to the continous nature of the results.<br>\n",
    "to make sure we output negative results as well as positive<br>\n",
    "a learning rate of 0.0001 is chosen due to percision and trial & error (with higher learning rates got stuck at local optima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(x_train.shape[1], input_shape=(x_train.shape[1], ), activation='tanh'),\n",
    "    tf.keras.layers.Dense(10, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(loss=haversine_loss,\n",
    "                optimizer=tf.keras.optimizers.Adam(0.00001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='max')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=50000, validation_data=(x_test, y_test), callbacks=[mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(list(range(len(old_history.history['loss']))), old_history.history['loss'], label='train_loss', c='r')\n",
    "plt.plot(list(range(len(old_history.history['loss']))), old_history.history['val_loss'], alpha=0.5, label='val_loss', c='b', ls='--')\n",
    "plt.ylabel('haversine loss')\n",
    "plt.xlabel('ephoch')\n",
    "plt.title('Learning curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"baseline_latlon.hdf5\", compile=False)\n",
    "reconstructed_model.compile(loss=haversine_loss,\n",
    "                optimizer=tf.keras.optimizers.Adam(0.00001))\n",
    "model = reconstructed_model\n",
    "pred = scaler_y.inverse_transform(model.predict(x_test))\n",
    "y_test_reg = scaler_y.inverse_transform(y_test)\n",
    "x_test_reg = scaler_x.inverse_transform(x_test)\n",
    "score_prev = haversine_50thP_95thP_mean(y_test_reg[:,0], y_test_reg[:,1], x_test_reg[:,0], x_test_reg[:,1])\n",
    "score_curr = haversine_50thP_95thP_mean(y_test_reg[:,0], y_test_reg[:,1], pred[:,0], pred[:,1])\n",
    "print(\"{}m error rate from our evaluation metric.\\n an impovement of {}m\".format(score_curr, score_prev-score_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latlon = np.array(X_val[['latDeg_bsln', 'lngDeg_bsln']])\n",
    "lossCalcHarvestine(val_truth_arr, latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_set = train[(train['collectionName_bsln'] == '2020-05-14-US-MTV-1')&(train['phoneName']=='Pixel4')]\n",
    "X_val = val_set.drop(['collectionName_bsln', 'collectionName_piv', 'phoneName', 'phone', 'epoch_timestamp_bsln', 'epoch_timestamp_piv', 'latDeg_grnd', 'lngDeg_grnd'], axis=1)\n",
    "X_val.fillna(0, inplace=True)\n",
    "val_truth = ground[(ground['collectionName'] == '2020-05-14-US-MTV-1')&(ground['phoneName']=='Pixel4')]\n",
    "X_val_norm = scaler.fit_transform(X_val)\n",
    "#val_truth_norm = scaler.fit_transform(val_truth)\n",
    "val_truth_arr = np.array(val_truth[['latDeg', 'lngDeg']])\n",
    "latlon = np.array(X_val)\n",
    "pred = model.predict(X_val_norm)\n",
    "pred = scaler_y.inverse_transform(pred)\n",
    "#compareRoutes(val_truth_arr, pred)\n",
    "compareRoutes(val_truth_arr, pred)\n",
    "IFrame(src='./map.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: finish regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Learning - simple regression model </h2>\n",
    "\n",
    "explain about the continous nature of the problem - small changes in satelite samples casue small changes in geospatial coordinates\n",
    "Then why is a regression model could be right for this case\n",
    "what is the cost function here and why\n",
    "\n",
    "problem description:\n",
    "input: all featuers in dervided that arent key columns\n",
    "output: lat, lng<br>\n",
    "<br>\n",
    "Use `data` to train the model to predict lat lon of `ground`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "first_grouped = data\n",
    "# drvd_grnd_match\n",
    "X = drvd_grnd_bsln.drop(['collectionName_drvd', 'collectionName_grnd', 'phoneName_drvd','phoneName_grnd', 'millisSinceGpsEpoch','signalType', 'latDeg', 'lngDeg'], axis=1)\n",
    "X = X.drop(['courseDegree', 'heightAboveWgs84EllipsoidM', 'hDop', 'speedMps', 'vDop', 'timeSinceFirstFixSeconds'], axis=1)\n",
    "\n",
    "Y = first_grouped[['latDeg', 'lngDeg']]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
