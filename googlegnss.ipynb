{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Smartphone Decimeter Challenge\n",
    "##### Improve high precision GNSS positioning and navigation accuracy on smartphones\n",
    "\n",
    "## Intro\n",
    "We are Nitzan Karni (208939215) and Shachar Helmer (318439833) both of us are toward the end of our computer science degree, and eager to specialize in data science. Both of us have a day to day interaction with data engineering, data analysis and even machine learning at times. In the data science world the possiblities are endless. While choosing a topic for this project countless of ambitious ideas were thrown into the air was - From old (19th century) picture colorization using Generative Advarsarial Networks to Radio Signal anomaly detection to natural language coding using GPT3 to translate english to code. Eventually with huge number of incomplete idea we got together and decided to decide once and for all. We browsed the web and kaggle for hours and then we encounter a fresh new challenge that had been uploaded at the same day: \"Google Smartphone Decimeter Challenge\", it was a sign from heavens! \n",
    "Due to the fact that Nitzan is working on his \"Autonomous cars and Swarm intelligence algorithms\" seminar in parallel and our common interest in Autonomous vehicles technologies - trying to solve the navigation positioning problem by using Data Science is the right fit for us.\n",
    "\n",
    "## About The Problem\n",
    "\n",
    "We believe there is no better explenation to the problem than the original challenge description: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/overview\n",
    "<br><br>\n",
    "Have you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.\n",
    "<br><br>\n",
    "Global Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a “jumpy” experience. For many use cases the results are not fine nor stable enough to be reliable.\n",
    "<br><br>\n",
    "This competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them.\n",
    "<br><br>\n",
    "In this competition, you'll use data collected from the host team’s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions.\n",
    "<br><br>\n",
    "If successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.\n",
    "<br>\n",
    "\n",
    "##### Our no. 1 objective\n",
    "Predict phone positioning at each sample to finest granuality possible from given measurements of GNSS system and other phone instruments such as accelometer & gyro. \n",
    "\n",
    "## The Data\n",
    "Our whole dataset in the competition is comprised of several data sources such as GNSS and phone insruments measurments.<br>\n",
    "All the data can be achieved using simple android smartphone.<br>\n",
    "The dataset we will use in this contest is consists of collections that have been collected separatly.<br>\n",
    "Each collection had been measured using the following method:<br>\n",
    "A car with at least one android device starts taking it's GNSS and phone insruments measurments and the Ground truth measurement reciever which will be explained in further details later on, drive around the city for a while and then stop the collection. <br>\n",
    "Most of the samples are from the silicon valley area around Google HQ.<br>\n",
    "The collections that had been collected over the course of approximatly one year.<br><br>\n",
    "An Example of a phone and reciever setup inside the car:\n",
    "![alt text](fig3_fig4.jpg \"An Example of a phone and reciever setup inside the car\")\n",
    "One trace consists of the following data for each android device that had been used in the process:<br>\n",
    "* `ground_truth.csv` - a csv file containing the correct specific positions in The WGS84 latitude, longitude (in decimal degrees) estimated by the reference GNSS receiver (NovAtel SPAN).<br>\n",
    "    the receiver is a preciese GNSS reciever, specific for the task piece of hardware that is installed in the car and can provide the exact locations of the car at each timestamp<br>\n",
    "    In addition to the GNSS receiver an IMU (Inertial Measurement Units) device is installed to accurratly measure the speed and courseDegree of the trace.<br>\n",
    "    The dataset consists the following features for each sample:\n",
    "    > `millisSinceGpsEpoch ` - An integer number of milliseconds since the GPS epoch (1980/1/6 midnight UTC). from our observations it seems that the samples are sampled at a 1s intervals.<br>\n",
    "    > `latDeg, lngDeg` - The latitude and longitude degrees from the center of the he WGS84 axis system. These features are out target features and we would like to predict these values.<br>\n",
    "    > ![alt text](280px-WGS_84_reference_frame_(vector_graphic).svg.png \"WGS_84 axis\")\n",
    "    > `heightAboveWgs84EllipsoidM ` - The WGS84 ellipsoid is a representation of earth (earth is actually round but more elliptical), the following feature represents the hight above the surface of the ellipsoid<br>\n",
    "    > `hDop, vDop` - Horizontal / Vertical dilution of precision. describes how errors in the measurements affect the final horizontal /vertical position estimation.  The idea of Geometric DOP is to state how errors in the measurement will affect the final state estimation.Conceptually you can geometrically imagine errors on a measurement resulting in the delta term changing<br>\n",
    "    > `speedMps` - The speed over ground in meters per second.<br>\n",
    "    > `courseDegree` - The course angle clockwise with respect to the truth north over ground (in degrees).\n",
    "    \n",
    "* `[phone_name]_GnssLog.txt` The GnssLogger App calls the google GNSS API and creates a text file containing at each file the full scale dataset for each sample<br>\n",
    "    - `Raw` - The raw GNSS measurements of one GNSS signal (each satellite may have 1-2 signals for L5-enabled smartphones), collected from the Android API GnssMeasurement.\n",
    "    - `Status` - The status of a GNSS signal, as collected from the Android API GnssStatus.\n",
    "    - `UncalAccel` -  Readings from the uncalibrated accelerometer\n",
    "    - `UncalGyro` - Readings from the uncalibrated gyroscope\n",
    "    - `UncalMag` - Readings from the uncalibrated magnetometer\n",
    "    - `OrientationDeg` - Each row represents an estimated device orientation\n",
    "    \n",
    "* `[phone_name]_derived.csv` - derived dataset is  GNSS intermediate values derived from raw GNSS measurements, provided for convenience. With these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite) can be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch.<br>\n",
    "    > A sample from android GNSS comprised of communication with several satelites.<br>\n",
    "    > For each satelite we are measuring several metrics here is an example of several important ones:<br>\n",
    "    > `collectionName` - The ID of the collection<br>\n",
    "    > `phoneName` - The name of phone<br>\n",
    "    > `svid` - The satelite id.<br>\n",
    "    > `millisSinceGpsEpoch`- millisSinceGpsEpoch in _derived.csv refers to the timestamp of the next epoch not the current epoch.<br>\n",
    "    > `rawPrM` - Raw pseudorange in meters. It is the product between the speed of light and the time difference from the signal transmission time (receivedSvTimeInGpsNanos) to the signal arrival time (Raw::TimeNanos - Raw::FullBiasNanos - Raw::BiasNanos).<br>\n",
    "    This is an important feature because actually for every measurement we can roughly calculate from the distance to the satelites and the satelites positions and velocities the exact point on earth surface using triangulation methods and intersect the data. we personally do not know much about the correct equations for this complex calculations but we can try later on using learning methods to generate a model based on these features.  \n",
    "    > `[x/y/z]SatPosM` - The satellite position (meters) in an ECEF coordinate frame at best estimate of “true signal transmission time” defined as ttx = receivedSvTimeInGpsNanos - satClkBiasNanos (defined below). They are computed with the satellite broadcast ephemeris, and have ~1-meter error with respect to the true satellite position.<br>\n",
    "    > `[x/y/z]SatVelMps` - The satellite velocity (meters per second) in an ECEF coordinate frame at the signal transmission time (receivedSvTimeInGpsNanos). They are computed with the satellite broadcast ephemeris, with this algorithm.<br>\n",
    "    > `constellationType` - GNSS constellation type. An integer number, whose mapping string value is provided in constellation_type_mapping.csv.<br>\n",
    "    > `signalType` - The GNSS signal type is a combination of the constellation name and the frequency band.<br>\n",
    "    > `receivedSvTimeInGpsNanos` - The signal transmission time received by the chipset.<br>\n",
    "\n",
    ".<br><br><br>`baseline_locations_[train/test].csv` - The following is the most similar to `ground_truth.csv` format. It is a tabular dataset which every table row represents a single measurement in one of our phones. That means that each row is refered to specific collection `collectionName`(for example Mountain View at 7/4/21) using specific phone `phoneName`(f.e. Pixel4) at specific timestamp`millisSinceGpsEpoch`. In addition the only features in the dataset are the features which exists in `ground_truth.csv`.<br>\n",
    "This dataset was generated using a standard Weighted Least Squares (WLS) approach run on the raw GNSS measurements. Similar WLS implementations can be found in RTKLib and the public version of Android GPS tools.\n",
    "<br><br>\n",
    "Each dataset has a train and test version except ground truth. <br>\n",
    "Using the test datasets we need to predict this value and submit our results.\n",
    "All datasets are present both for training data as well as for the test data\n",
    "\n",
    "For further reading on the Data please refer to: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/data\n",
    "\n",
    "### Some Domain Knowledge\n",
    "Earth is surrounded by navigation satelite systems.<br>\n",
    "Overall there are 4 global navigation systems and 2 local, ones around India and most of south Asia and another japanese one that cover most of east Asia and Oceania (Pacific Ocean and Australia region).<br>\n",
    "Each sample also contains satelite atomic clock metrics and several parameters that can affect the delay and noise of the measurements such as clock drift, ionospheric layer delay, tropospheric layer delay (Layers of earth's sky that can cause measurement deviation, similar to the well known Atmosphere layer).<br>\n",
    "Essentially the phone will try and communicated with as many satelites it can reach from its position. The more the merrier.<br>In our final calculation the more data we can interpolate regarding the positioning of the phone the finer our result would get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import gmplot\n",
    "from IPython.display import IFrame\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import simdkalman\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce\n",
    "import math\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Dense, ReLU, LeakyReLU, Concatenate, Lambda, Input\n",
    "import json\n",
    "import sys\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate through the folder structure to setup our derived & ground truth dataframes.<br>\n",
    "The datasets are ordered in file hierarchy and the following methods are going through the collections and generate one unified<br>\n",
    "dataset, it is needed for the ground_truth dataset (the target variable) and for the derived data (the data which we are going to train our model on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_files_to_df(path_list):\n",
    "    \"\"\"\n",
    "    Read a list of structured files in csv format, concatenating them into a single DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_list: list of paths to csv datasets\n",
    "    \"\"\"\n",
    "    return reduce(lambda df1, df2: pd.concat([df1, df2]), [pd.read_csv(s) for s in path_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simplify_set_file_name(file_name):\n",
    "    \"\"\"\n",
    "    Convert the collection name to human readble format according to:\n",
    "    [train]/[drive_id]/[phone_name]/\n",
    "    \"\"\"\n",
    "    path_rest = file_name.split('google-smartphone-decimeter-challenge/')[1]\n",
    "    trn_grnd = path_rest.split('/')[0]\n",
    "    path_rest = path_rest.replace(f'{trn_grnd}/', '')\n",
    "    date = path_rest.split('/')[0]\n",
    "    path_rest = path_rest.replace(f'{date}/', '')\n",
    "    phone = path_rest.split('/')[0]\n",
    "    return f'{trn_grnd} | {date} | {phone}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the list of datasets needed to be red into our project\n",
    "We will check how many train collections exists compared to the test ones.<br>\n",
    "In addition we will check the presence of the features in the datasets that we have read in order to avoid situations where a specific collection somehow missing a feature.<br>\n",
    "Each unique GPS sample has a varying amount of satelite samples describing it (rows in the derived data set), yet they all refer to the same time-location combination.</br>\n",
    "In the ground truth data set, samples are matched with rows in an \"on\" and \"one-to-one\" fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_data_dir = 'google-smartphone-decimeter-challenge'\n",
    "\n",
    "train_collections = [f'{base_data_dir}/train/{collection}' for collection in os.listdir(f'{base_data_dir}/train')]\n",
    "test_collections = [f'{base_data_dir}/test/{collection}' for collection in os.listdir(f'{base_data_dir}/test')]\n",
    "\n",
    "\n",
    "print(f'{len(train_collections)} train collections, {len(test_collections)} test collections\\n')\n",
    "\n",
    "derived_train_sets = [f'{c}/{s}/{s}_derived.csv' for c in train_collections for s in os.listdir(c)]\n",
    "ground_train_sets = [f'{c}/{s}/ground_truth.csv' for c in train_collections for s in os.listdir(c)]\n",
    "derived_test_sets = [f'{c}/{s}/{s}_derived.csv' for c in test_collections for s in os.listdir(c)]\n",
    "logs_test_sets = [f'{c}/{s}/{s}_GnssLog.txt' for c in test_collections for s in os.listdir(c)]\n",
    "logs_train_sets = [f'{c}/{s}/{s}_GnssLog.txt' for c in train_collections for s in os.listdir(c)]\n",
    "\n",
    "\n",
    "drvd_trn_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in derived_train_sets])\n",
    "grnd_trn_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in ground_train_sets])\n",
    "drvd_tst_clms = reduce(lambda s1, s2: s1.union(s2), [set(list(pd.read_csv(s, nrows=1).columns)) for s in derived_test_sets])\n",
    "print('Do all columns appear in all non-log data sets?')\n",
    "print(f'Derived train data: {drvd_trn_clms == set(list(pd.read_csv(derived_train_sets[0], nrows=1).columns))}')\n",
    "print(f'Ground truth train data: {grnd_trn_clms == set(list(pd.read_csv(ground_train_sets[0], nrows=1).columns))}')\n",
    "print(f'Derived test data: {drvd_tst_clms == set(list(pd.read_csv(derived_test_sets[0], nrows=1).columns))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one unified dataset for derived ground truth data for ease of access and better training process.<br>\n",
    "Generate one unified dataset for derived train data for ease of access and better training process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived = read_files_to_df(derived_train_sets)\n",
    "ground = read_files_to_df(ground_train_sets)\n",
    "print(\"# of samples in ground truth train dataset: {}\\n# of samples in derived train dataset: {}\\n\".format(derived.shape[0], ground.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ground.pkl', 'rb') as f:\n",
    "    ground = pickle.load(f)\n",
    "with open('derived.pkl', 'rb') as f:\n",
    "    derived = pickle.load(f)\n",
    "with open('androind-measurements.pkl', 'rb') as f:\n",
    "    logs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the baseline train and test data.<br>\n",
    "For further analysis create a target dataset which is simply ground thruth merged with the baseline predictions to align the samples using the ['collectionName', 'phoneName', 'millisSinceGpsEpoch'] index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm=['collectionName', 'phoneName', 'millisSinceGpsEpoch'] # simple way to access this combination of columns that would be frequently accessed\n",
    "bsln_trn = pd.read_csv('google-smartphone-decimeter-challenge/baseline_locations_train.csv')\n",
    "bsln_tst = pd.read_csv('google-smartphone-decimeter-challenge/baseline_locations_test.csv')\n",
    "target = ground.merge(bsln_trn, how='inner', on=cpm, suffixes=('_grnd', '_bsln'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to utilize the raw GNSS log files as well, we would want to match the different measurements to the respective time samples they represent in the ground baseline and derived datasets. This is done using the same triple: Epoch, collection name, phone name. So we define methods to parse the GNSS logs, and also an iteration over all logs that adds the collection and phone names to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnss_log_to_dataframes(path):\n",
    "    \"\"\"\n",
    "    Parse a gnss log file and read it into a dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: location of the gnss log to parse\n",
    "    \"\"\"\n",
    "    gnss_section_names = {'Raw','UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'}\n",
    "    with open(path) as f_open:\n",
    "        datalines = f_open.readlines()\n",
    "\n",
    "    datas = {k: [] for k in gnss_section_names}\n",
    "    gnss_map = {k: [] for k in gnss_section_names}\n",
    "    error_count = 0\n",
    "    for dataline in datalines:\n",
    "        try:\n",
    "            is_header = dataline.startswith('#')\n",
    "            dataline = dataline.strip('#').strip().split(',')\n",
    "            # skip over notes, version numbers, etc\n",
    "            if is_header and dataline[0] in gnss_section_names:\n",
    "                gnss_map[dataline[0]] = dataline[1:]\n",
    "            elif not is_header:\n",
    "                datas[dataline[0]].append(dataline[1:])\n",
    "        except KeyError as e:\n",
    "            error_count += 1            \n",
    "\n",
    "    results = dict()\n",
    "    for k, v in datas.items():\n",
    "        results[k] = pd.DataFrame(v, columns=gnss_map[k])\n",
    "    # pandas doesn't properly infer types from these lists by default\n",
    "    for k, df in results.items():\n",
    "        for col in df.columns:\n",
    "            if col == 'CodeType':\n",
    "                continue\n",
    "            results[k][col] = pd.to_numeric(results[k][col])\n",
    "\n",
    "    if error_count > 0:\n",
    "        print(f'{path} showed a {error_count} key error(s). Some log data rows for it might not be available.')\n",
    "    return results\n",
    "\n",
    "def read_log(path, log_sections=['Raw', 'UncalAccel', 'UncalGyro', 'UncalMag']):\n",
    "    \"\"\"\n",
    "    Takes in a gnss log file for a collection, parses it and outputs a single dataframe containing all properties specified (or the default ones)\n",
    "    from that gnss log, if they exist. They are merged based on the 'utcTimeMillis' field in an 'outer' fashion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: the path to the gnss log file\n",
    "    log_properties: the sub-sections to utilize from the log\n",
    "    \"\"\"\n",
    "    log = gnss_log_to_dataframes(path)\n",
    "    # Elapsed real time nanos is cut off since it is not needed and reapeats in multiple sections (to prevent merge suffixes)\n",
    "    return reduce(lambda df1, df2: pd.merge_asof(df1, df2, on=['utcTimeMillis'], suffixes=['','']),\n",
    "                  [log[section][[c for c in list(log[section].columns) if c != 'elapsedRealtimeNanos']].sort_values('utcTimeMillis') for section in log_sections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLogs(logs_sets):\n",
    "    logs_sets_details = [(s, s.split('/')[2], s.split('/')[3]) for s in logs_sets]\n",
    "    logs_dfs = []\n",
    "    for s in logs_sets_details:\n",
    "        set_df = read_log(s[0])\n",
    "        set_df['collectionName'] = s[1]\n",
    "        set_df['phoneName'] = s[2]\n",
    "        logs_dfs.append(set_df)\n",
    "    return reduce(lambda df1, df2: pd.concat([df1, df2]), logs_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = readLogs(logs_test_sets)\n",
    "print(\"# of records in all GNSS test logs: {}\\n\".format(logs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = readLogs(logs_train_sets)\n",
    "print(\"# of records in all GNSS train logs: {}\\n\".format(logs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(derived, 'derived.pkl')\n",
    "pd.to_pickle(ground, 'ground.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(logs, 'androind-measurements.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching this GNSS log data based on the epoch time a little tricky here, becuase the measurements give us the time in utc epochs.</br>\n",
    "utcTimeMillis refers to unix time - a method to describe seconds in time, by the number of seconds passed since the unix epoch (00:00:00 UTC 1/1/1970).</br>\n",
    "The other datasets use gps epoch times - the same idea but this time describing seconds passed since the GNSS epoch (00:00:00 5/1/1980). </br></br>\n",
    "Matching time measured by both the methods would be easy as subtracting the known difference between their start dates, but it isn't as simple since GPS time is a</br>\n",
    "continous time scale - it doesn't take into account leap seconds. </br>\n",
    "Leap seconds are adjusments when measuring in UTC, to accommodate the difference between precise time (as measured by atomic clocks) and imprecise observed solar time</br> (known as UT1 and which varies due to irregularities and long-term slowdown in the Earth's rotation).</br>\n",
    "We see this when we look at the GNSS logs' 'raw' section which has a 'LeapSecond' field specifying how big was the leap differnece when that measurement was taken.</br>\n",
    "So we'll use utcTimeMillis and LeapSecond to generate a GPS time valid timestamp, along with all the other wanted log features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_features_used = ['utcTimeMillis', 'UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2', 'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec',\n",
    "                    'UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT', 'LeapSecond', 'collectionName', 'phoneName']\n",
    "logs = logs[log_features_used]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to verify that the logs contain actual leap second data we get a disappointing result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are: {logs[\"LeapSecond\"].notnull()[lambda r: r].count()} records containing leap second information out of {logs.shape[0]} log records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: This is confusing. Some places say that this leap second information is already accounted for in the utc time...</br>\n",
    "This anrdoid doc says that it might not be available always: https://developer.android.com/reference/android/location/GnssClock#hasLeapSecond()</br>\n",
    "This answer says that some places already include leap seconds: https://stackoverflow.com/a/44315709/10047211</br>\n",
    "This is also very useful https://confluence.qps.nl/qinsy/latest/en/utc-to-gps-time-correction-32245263.html</br>\n",
    "From the last link I understand that leap seconds matter, but they move together, so for the difference between them it might not matter at all.....\n",
    "also if the leap seconds field is empty, we can retrieve it from the internet for the 2018-2021 with ease\n",
    "**End of TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a constant difference between the two epochs, putting aside the leap seconds. </br>\n",
    "It is exactly 315964782 seconds (**TODO: assert me**). So getting the gps epoch for the logs is as simple as subtracting the amount of time utc is ahead of gps time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_gps_diff = 315964782\n",
    "logs['millisSinceGpsEpoch'] = logs['utcTimeMillis'] - (utc_gps_diff * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging the two dataset we will check for duplicated rows in the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_dup = logs.duplicated().sum()\n",
    "print(\"# of duplicated rows over all columns in the logs dataset is: {} rows\".format(num_of_dup))\n",
    "logs.drop_duplicates(inplace=True)\n",
    "print(\"# of rows after duplicated had been removed: {} rows\".format(logs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to merge this log data with the baseline set, to assert it is valid for use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use merge asof using tolerance of 1 seconds due to a diviation of 1 second that is probably exists due to rounding to millis error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs['epoch']=logs['millisSinceGpsEpoch']\n",
    "logs_bsln = pd.merge_asof(bsln_trn.sort_values('millisSinceGpsEpoch'), logs.sort_values('millisSinceGpsEpoch'),on='millisSinceGpsEpoch', \n",
    "                          by=['collectionName', 'phoneName'], tolerance=1, direction='backward')\n",
    "print(\"# of rows which did not merged properly is: {}\".format(logs_bsln[logs_bsln['millisSinceGpsEpoch']-logs_bsln['epoch']>1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn = logs_bsln.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------\n",
    "# Exploratory Data Analysis\n",
    "### EDA on derived dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "First of all we want to look at what we even want to predict.<br>\n",
    "Therefore we will have some vizualizations that will help us understand the ground truth dataset<br>\n",
    "and our target variables latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareRoutes(true_route, predicted_route=None, mapName='map', color='cornflowerblue'):\n",
    "    \"\"\"Recieve as input two routes one as the ground truth and the other as the predicted.\n",
    "    The ground truth would be drawn as line on the map.\n",
    "    The predicted would be drawn as heatmap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_route : numpy 2d array\n",
    "        The true route the sample has went through\n",
    "    predicted_route : numpy 2d array\n",
    "        The predicted route\n",
    "    mapName : str\n",
    "        defualt = 'map'\n",
    "        The name of the generated html google map\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    mapName.html file containing the google map.\n",
    "    \"\"\"\n",
    "    gmap1 = gmplot.GoogleMapPlotter(true_route[true_route.shape[0]//2,0], true_route[true_route.shape[0]//2,1], 14, apikey='AIzaSyB0ONxmQBgtM14DqTRDrYBBUw2-woWkCIE', map_type='hybrid')\n",
    "    gmap1.plot(true_route[:,0],true_route[:,1], color, edge_width=2)\n",
    "    if(predicted_route is not None):\n",
    "        gmap1.heatmap(predicted_route[:,0],predicted_route[:,1])\n",
    "    gmap1.draw( \"{}.html\".format(mapName) )\n",
    "    # Print the map to notebook\n",
    "    return IFrame(src=\"./{}.html\".format(mapName), width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_route = ground[['latDeg', 'lngDeg']]\n",
    "mapName='map'\n",
    "plasma = matplotlib.cm.get_cmap('plasma',ground['collectionName'].nunique())\n",
    "true_route = np.array(ground[['latDeg', 'lngDeg']])\n",
    "gmap1 = gmplot.GoogleMapPlotter(true_route[true_route.shape[0]//2,0], true_route[true_route.shape[0]//2,1], 10, apikey='AIzaSyB0ONxmQBgtM14DqTRDrYBBUw2-woWkCIE', map_type='hybrid')\n",
    "for i, collection in enumerate(ground['collectionName'].unique()):\n",
    "    color = matplotlib.colors.rgb2hex(plasma.colors[i])\n",
    "    df = ground[ground['collectionName']==collection]\n",
    "    df = df[df['phoneName'] == df.iloc[0]['phoneName']]\n",
    "    true_route = np.array(df[['latDeg', 'lngDeg']])\n",
    "    gmap1.plot(true_route[:,0],true_route[:,1], color, edge_width=5)\n",
    "gmap1.draw( \"{}.html\".format(mapName) )\n",
    "# Print the map to notebook\n",
    "IFrame(src=\"./{}.html\".format(mapName), width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following map protrayes the data collection traces over the bay area, as we can see most of the traces<br>\n",
    "are from the same area, there are even some traces that overlap.<br>\n",
    "this fact can help us train a model that is specificaly made for the bay area.<br>\n",
    "there are some more complex routes than others, some are on the highway while others are driving in the city.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot we are going to plot the collections durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "df = ground.groupby('collectionName').agg({'millisSinceGpsEpoch': [('duration (minutes)', lambda x: ((np.max(x)-np.min(x))/1000)/60)]}).reset_index()\n",
    "g = sns.barplot(data=df,x=('millisSinceGpsEpoch', 'duration (minutes)'), y='collectionName')\n",
    "g.axvline(float(df.mean()), label='mean duration')\n",
    "g.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the collections are around 30 minutes of driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets portray the change in speed over time for all the collections to have some understanding on the consistency of the speed.<br>\n",
    "the more the speed will change the harder it gets to predict the data using simple models that their main assumption is that speed won't change<br>\n",
    "therefore it can be easy for them to predict the vehicle course of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerCollection(data, collections, features, figsize=(15, 12)):\n",
    "    cols=6\n",
    "    rows = (len(collections)//6)+1\n",
    "    if(len(collections)<6):\n",
    "        cols = len(collections)\n",
    "    fig, ax = plt.subplots(nrows= rows, ncols=cols, sharex=True, sharey=True, figsize=figsize)\n",
    "    for i in range(ax.shape[0]):\n",
    "        if(len(ax.shape) == 1):\n",
    "            df = data[data['collectionName']==collections[i]]\n",
    "            df = df[df['phoneName'] == df.iloc[0]['phoneName']]\n",
    "            for feature in features:\n",
    "                ax[i].plot((df['millisSinceGpsEpoch']-df.iloc[0]['millisSinceGpsEpoch'])/1000, df[feature], label=feature)\n",
    "            ax[i].title.set_text(collections[i])\n",
    "        else:\n",
    "            for j in range(ax.shape[1]):\n",
    "                df = data[data['collectionName']==collections[i*ax.shape[0]+j]]\n",
    "                df = df[df['phoneName'] == df.iloc[0]['phoneName']]\n",
    "                for feature in features:\n",
    "                    ax[i][j].plot((df['millisSinceGpsEpoch']-df.iloc[0]['millisSinceGpsEpoch'])/1000, df[feature], label=feature)\n",
    "                ax[i][j].title.set_text(collections[i*ax.shape[0]+j])\n",
    "    fig.text(0.5, 0.04, 'Timestamp in seconds since the start of the collection', ha='center', size='large')\n",
    "    fig.text(0.08, 0.5, str(features)+\" features values\", va='center', rotation='vertical', size='large')\n",
    "    if(len(ax.shape) != 1):\n",
    "        handles, labels = ax[i][j].get_legend_handles_labels()\n",
    "    else:\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center')\n",
    "plotPerCollection(ground, ground['collectionName'].unique(), ['speedMps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seen from the map indeed we have collections which have more consistent speed<br>\n",
    "due to the fact that they are simply driving a car on a highway probably without changing lanes too much.<br>\n",
    "We believe these collections will be far easier to predict with great accurracy than the ones where the speed is constantly changing.<br>\n",
    "The collections which the speed is constantly changing are related to inner city collections such as in 2021-04-22-US-SJC-1 which is a low speed measurement<br>\n",
    "but constantly the car comming to a full stop due to the multiple turns.<br>\n",
    "Later on when we will compare the baseline predictions to the ground truth we will inspect whether the speed change rate do have an effect on the performance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerCollection(ground, ground['collectionName'].unique(), ['hDop', 'vDop', 'heightAboveWgs84EllipsoidM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visible navigation satellites are close together in the sky, the geometry is said to be weak and the DOP value is high; when far apart, the geometry is strong and the DOP value is low.<br>\n",
    "Thus a low DOP value represents a better positional precision due to the wider angular separation between the satellites used to calculate a unit's position. Other factors that can increase the<br>effective DOP are obstructions such as nearby mountains or buildings.<br> DOP can be expressed as a number of separate measurements. HDOP, VDOP, PDOP, and TDOP are respectively Horizontal, Vertical, Position (3D), and Time Dilution of Precision.<br>\n",
    "https://www.gsat.us/support/glossary/hdop\n",
    "\n",
    "We conclude that the lower the DOP value the more precise positioning. (maybe can be used as a way to give weight to our samples)<br>\n",
    "another thing we understand is that the obsticles around us can affect the DOP values therefore we would like to plot the height which can give<br>\n",
    "us indication regarding mountainess region.\n",
    "\n",
    "As we can notice from the plot above most of the higher DOP values occurress around valleys.<br>\n",
    "a perfect example for this phenomenon is collection 2021-04-15-MTV-US-1, the hDOP value spikes whenever the vehicle reaches the bottom of the vally and is blocked by high mountains.<br>\n",
    "The same is happening at 2020-08-03-MTV-US-1 and more. It is clear that whenever the montains are blocking the GNSS signal we are getting dilution in the precision.<br>\n",
    "In the city collections the region is not blocked by montains rather by buildings therefore these spikes of DOP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline dataframe analysis\n",
    "As we mentioned before the baseline dataframe had been generated by weighted least squares on the derived data.<br>\n",
    "we will describe later the derived data more deeply but due to it's complexisity of understanding and the need for domain knowledge we would like to<br>\n",
    "firstly talk about the baseline dataset which is far more easier to understand.\n",
    "<br><br>\n",
    "Two images that help us understand why we even need to be more accurrate with our predictions:<br>\n",
    "![alt text](BaselineFuckesup.jpg \"under the bridge no reception\")\n",
    "<br>it is clear from the following image that there is a huge problem with the baseline predictions.<br>\n",
    "there are some predictions that the baseline positioning us outside the road and even up on the hill as far as at least 5m from the car position.<br>\n",
    "in other data points it positioning us on the opposite lane.<br>\n",
    "![alt text](WhyWeNeedMoreAccurateGPS.jpg \"In the building\")\n",
    "<br>In this example baseline is positioning us on the walking pavement and even on the rooftops.<br>\n",
    "very inaccurrate positioning that any application that need some more refined positioning cannot use these predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(bsln_trn.sort_values(cpm).reset_index()[cpm] == ground.sort_values(cpm).reset_index()[cpm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn.sort_values(cpm, inplace=True)\n",
    "ground.sort_values(cpm, inplace=True)\n",
    "\n",
    "def isSynched(left, right):\n",
    "    \"\"\"\n",
    "    Checks if two datasets are synched by the collection-phone-epoch\n",
    "    \"\"\"\n",
    "    return np.all(left.sort_values(cpm).reset_index()[cpm] == right.sort_values(cpm).reset_index()[cpm])\n",
    "\n",
    "if isSynched(bsln_trn, ground):\n",
    "    print(\"Baseline and ground truth data are synchronized\")\n",
    "else:\n",
    "    print(\"Baseline and ground truth data are not synchronized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice there is not much data in the baseline but it is exactly the prediction that is needed and our model would need to perform at least better than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot collection route on a map using ground truth route vs. baseline route\n",
    "The following function draws the ground truth locations on google map map and on the ground truth locations the map plots a heatmap of the frequency of the locations of baseline location predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following map we can see the baseline predicted values vary around the ground truth<br>\n",
    "The route in blue represent the true route of the measurements while the hit map represent the baseline measurement.<br>\n",
    "The hotter the color gets, the more frequent and concentraited the predictions are.<br>\n",
    "You can interact with the map as with any google map. <br>\n",
    "Try to zoom in and have a look at the prediction distribution around the route and how it diviates from the ground truth<br>\n",
    "We can also notice why decimeter prediction is neccessary as jumpy navigation system can cause quite distress,<br>\n",
    "Especially if we finding ourself outside the road or even on the roof of some building as we can observe in multiple observations.<br>\n",
    "High accurracy navigation systems such as ADAS, Autonomous vehicles or even Pokemon GO have critical neccessity in highly accurrate lat lng positioning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_bsln[logs_bsln.drop(['LeapSecond'], axis=1).isna().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drawRandomPath(collection='random', phone='random', mapName='random'):\n",
    "    if collection == 'random':\n",
    "        collection = target['collectionName'].sample().iloc[0]\n",
    "    if phone == 'random':\n",
    "        phone = target[target['collectionName'] == collection]['phoneName'].sample().iloc[0]\n",
    "    t = target[(target['collectionName'] == collection) & (target['phoneName'] == phone)]\n",
    "    print(phone, collection)\n",
    "    return compareRoutes(np.array(t[['latDeg_grnd','lngDeg_grnd']]), np.array(t[['latDeg_bsln','lngDeg_bsln']]), mapName=mapName)\n",
    "drawRandomPath(collection='2020-09-04-US-SF-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drawRandomPath(collection='2021-04-29-US-MTV-1', phone='SamsungS20Ultra', mapName='2021-04-29-US-MTV-1-SamsungS20Ultra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawRandomPath(collection='2021-04-29-US-MTV-1', phone='Pixel4', mapName='2021-04-29-US-MTV-1-Pixel4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we compare two devices on the same route it seems the pixel 4 is a bit more cluttered than the samsung device.<br>\n",
    "But both of them are off from the real lane for at least 2m if not more at complex turns and when the vehicle stops.<br>\n",
    "Every intersection where the vehicle stopped we can notice a big cloud of data points for both of the devices<br>\n",
    "Maybe the baseline method is more prone to error at changes in the vehicle speed and low speed traveling.<br>\n",
    "It seems that at segments of the road where the car should have constant speed it seems like the datapoints are more accurate.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_tmp = ground.copy()\n",
    "ground_tmp['acceleration'] = (ground['speedMps'] - ground['speedMps'].shift()) / (ground['millisSinceGpsEpoch'] - ground['millisSinceGpsEpoch'].shift())\n",
    "ground_tmp[ground_tmp['acceleration']>0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerCollection(bsln_trn.fillna(0), bsln_trn['collectionName'].unique(), ['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerCollection(bsln_trn.fillna(0), bsln_trn['collectionName'].unique(), ['UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accelaration in the z axis is always around 9.81, which is earth's gravity accelaration.<br>\n",
    "mostly the accelaration are keeping it constant around 0 and g and diverge around that value.<br>\n",
    "there are some collections which are almost steady on the accelaration while other constantly shifting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is a good performance? how do we measure it?<br>\n",
    "How can we tell that the baseline predictions are not good enough for precise navigation and accurrate positioning?<br>\n",
    "this rough comparison between the ground truth and the baseline predictions can't tell us nothing about how well the least squares mehod performed.<br>\n",
    "We need numerical evaluation for the performance of the predictions.<br>\n",
    "luckily the competition evaluation metric is provided for us<br>\n",
    "therefore we think that before we continue analysing any training data we should at least display why baseline predictions are just not good enough<br>\n",
    "and we need to get much more accurrate<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the evaluation metric\n",
    "The challenge evaluation metric is set to be as following:<br>\n",
    "https://www.kaggle.com/c/google-smartphone-decimeter-challenge/overview/evaluation<br>\n",
    "Submissions are scored on the mean of the 50th and 95th percentile distance errors. For every phone and at every millisSinceGpsEpoch,<br> the horizontal distance (in meters) is computed between the predicted lat/lng and the ground truth lat/lng.<br> These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller).<br> The 50th and 95th percentile errors are then averaged for each phone.<br> Lastly, the mean of these averaged values is calculated across all phones in the test set.<br><br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Haversine_formula\n",
    "\n",
    "The harversine function determines the \"Great circle\" distance between 2 latlon datapoints.<br>\n",
    "We are using this measurement to accurratly determine as needed the `horizontal distance` between two points on earth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition metric that is required is calculating the distances between the true and predicted values.<br>\n",
    "Then return the mean of the median and 95th percentile out of that distance vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def haversine_50thP_95thP_mean(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Using calc_haversine formula to calculate the mean of  the mean of the 50th and 95th percentile distance errors\n",
    "    The Competition evaluation metric\n",
    "    \"\"\"\n",
    "    haversine = calc_haversine(lat1, lon1, lat2, lon2)\n",
    "    return (np.percentile(haversine, 95) + np.median(haversine)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are set to go with already predicted dataset of samples.<br>\n",
    "What is so bad with the initial predictions that we even need to make an effort to minimize its error?<br>\n",
    "Let's have a look at the competition evaluation for the given predictions and try to asses how they differ from the ground truth.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_prev = haversine_50thP_95thP_mean(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln'], target['lngDeg_bsln'])\n",
    "print(\"{}m is the mean of the meadian error and the 95th percentile\".format(score_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As implied in the compition name 6.2 meters is simply not enough.<br>\n",
    "we have to reduce the prediction error to at least under 1m to get into the decimeter realm.<br>\n",
    "6 meters doesn't seem to be that far but soon we would see it is not an easy task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's continue our invistigation using the evaluation metric\n",
    "With the haversine calculation we can compare the baseline estimated coordinates to the ground truth, sliced by various columns. Thus we might get an intuition for more and less \"accurate\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drvd_grnd_match = pd.merge_asof(derived.sort_values('millisSinceGpsEpoch'), ground.sort_values('millisSinceGpsEpoch'), on='millisSinceGpsEpoch',\n",
    "              suffixes=['_drvd', '_grnd'], tolerance=1)\n",
    "drvd_grnd_bsln = pd.merge_asof(drvd_grnd_match, bsln_trn.sort_values('millisSinceGpsEpoch'), on='millisSinceGpsEpoch',\n",
    "                              suffixes=['','_bsln'], tolerance=1)\n",
    "\n",
    "drvd_grnd_bsln['hvr_dist'] = calc_haversine(drvd_grnd_bsln.latDeg, drvd_grnd_bsln.lngDeg, drvd_grnd_bsln.latDeg_bsln, drvd_grnd_bsln.lngDeg_bsln)\n",
    "\n",
    "def evaluate_from_dist(hvr_dist):\n",
    "    return (np.nanpercentile(hvr_dist, 95) + np.nanmedian(hvr_dist)) / 2\n",
    "\n",
    "cltn_hvr_dist = drvd_grnd_bsln.groupby(['collectionName','phoneName'])['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "phone_hvr_dist = drvd_grnd_bsln.groupby('phoneName')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "sig_type_hvr_dist = drvd_grnd_bsln.groupby('signalType')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "svid_hvr_dist = drvd_grnd_bsln.groupby('svid')['hvr_dist'].agg(evaluate_from_dist).reset_index()\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(18, 11, forward=True)\n",
    "axs[0][0].set_title('collections compared with the haversine distance their samples produce')\n",
    "sns.barplot(x='hvr_dist', y='collectionName', data=cltn_hvr_dist, ax=axs[0][0])\n",
    "axs[0][1].title.set_text('Phone names compared with the haversine distance their samples produce')\n",
    "sns.barplot(x='hvr_dist', y='phoneName', data=phone_hvr_dist, ax=axs[0][1])\n",
    "axs[1][0].title.set_text('Signal types compared with the haversine distance that samples of that type produce')\n",
    "sns.barplot(x='hvr_dist', y='signalType', data=sig_type_hvr_dist, ax=axs[1][0])\n",
    "axs[1][1].title.set_text('Satelite ID compared with the haversine distance that its samples produce')\n",
    "sns.barplot(y='hvr_dist', x='svid', data=svid_hvr_dist, ax=axs[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An interesting outcome! Some features yield a greater deviation in haversine distance values. <br>We can immidiatly learn about two very problematic collections.<br>\n",
    "`2021-04-22-US-SJC-1` and `2021-04-29-US-SJC-2`, could it be harder to predict in San Jose? <br>\n",
    "- Another interesting finding is the error score of `SamsungS20Ultra` which is by far much more worst than the other phones.<br>\n",
    "- Phones using BeiDu (Chinese) signalType are experiencing slightly worst GNSS performance<br>\n",
    "- Satelites 34 & 37 are noticably worst performers than the rest of the bunch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the san jose collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawRandomPath(collection='2021-04-22-US-SJC-1', mapName='2021-04-22-US-SJC-1-any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it seems kind of obvious that we will have the most trouble here. as mentioned before high rise buildings can cause GNSS signal interferences.<br>\n",
    "Another thing that may be problematic is the high rate of changes in speed which can be cause for inconsistency.<br>\n",
    "the following plot can show us better exactly the rate of change in the phone accelaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPerCollection(bsln_trn.fillna(0), ['2021-04-22-US-SJC-1', '2021-04-29-US-SJC-2', '2020-05-14-US-MTV-1'], ['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2'], figsize=(16,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both the worst and the best performing collections are constantly changing acceleration<br>\n",
    "but there is a difference. the best performing data seems much more full than the worst. <br>\n",
    "the accelaration graph over time looks more continous and less breakapart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bsln_trn[bsln_trn['collectionName']=='2021-04-22-US-SJC-1'].drop(['LeapSecond'], axis=1).isna().any(axis=1).sum()/len(bsln_trn[bsln_trn['collectionName']=='2021-04-22-US-SJC-1'])\n",
    "print(\"{:.2f}% of San Jose datapoints are missing accelaration values from the logs\".format(p*100))\n",
    "p = bsln_trn[bsln_trn['collectionName']=='2020-05-14-US-MTV-1'].drop(['LeapSecond'], axis=1).isna().any(axis=1).sum()/len(bsln_trn[bsln_trn['collectionName']=='2020-05-14-US-MTV-1'])\n",
    "print(\"{:.2f}% of Mountain View datapoints are missing accelaration values from the logs\".format(p*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the collections has very low null occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2021-04-22-US-SJC-1 phones are {}\".format(list(bsln_trn[bsln_trn['collectionName']=='2021-04-22-US-SJC-1']['phoneName'].unique())))\n",
    "print(\"2020-05-14-US-MTV-1 phones are {}\".format(list(bsln_trn[bsln_trn['collectionName']=='2020-05-14-US-MTV-1']['phoneName'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it is simply the phone that affects the performance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "sns.barplot(x='hvr_dist', y='collectionName', data=cltn_hvr_dist[cltn_hvr_dist['collectionName'].isin(['2021-04-22-US-SJC-1', '2020-05-14-US-MTV-1'])], hue='phoneName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the phone do not have anything to do with the performance between these two collections (pixel4 is performing even worst in San Jose).<br>\n",
    "simply put it is probably the fault in the buildings around the town that blocks the reception to the phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline vs. ground truth spatial comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_distance'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln'], target['lngDeg_bsln'])\n",
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'])\n",
    "plt.ylabel('haversine error in decimeters')\n",
    "plt.xlabel('datapoint index number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the error is quite large using simply the baseline data (Weighted Least Squars on derived data).<br>\n",
    "Further more we have some very strange outlier measurements that with very large errors that are unacceptable at any circumstances and do not even imply on simple GPS accurracy.<br> These outlier samples can range all the way from 40m error to 8km error.<br> In the future we would use outlier detection algorithm to detect these samples and then clean them.<br>\n",
    "\n",
    "Our mission is to flat that plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = (target['bsln_grnd_distance']<0.5).apply(lambda x: 'g' if x else 'r')\n",
    "size = (target['bsln_grnd_distance']<0.5).apply(lambda x: 2 if x else 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121); plt.title('Baseline vs. ground truth latDeg'); \n",
    "plt.scatter(target['latDeg_grnd'], target['latDeg_bsln'], color=colors, s=size)\n",
    "plt.xlabel('ground truth latDeg'); plt.ylabel('baseline latDeg')\n",
    "plt.subplot(122); plt.title('Baseline vs. ground truth lngDeg');\n",
    "plt.scatter(target['lngDeg_grnd'], target['lngDeg_bsln'], color=colors, s=size)\n",
    "plt.xlabel('ground truth lngDeg'); plt.ylabel('baseline lngDeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we can see two plots. One for latDeg and the other for lngDeg.<br>\n",
    "Although it seems like a line it is a scatter plot, each point would be colored in green if the horizontal distance<br>\n",
    "from baseline prediction is lower than 5 decimeters otherwise in red. our ambision is to make transform the plot to green line that corripond to the y=x line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived dataset\n",
    "After going through the ground data to know what we want to predict and going through baseline to show us that the baseline method is simply not enough.<br>\n",
    "We can choose to use only the baseline GNSS data combined with the phone other sensors or we can choose to go to the origins and try use the raw basic measurements from<br>\n",
    "the GNSS system.<br>\n",
    "It is worth saying that derived dataset is much more complicated and overall we wouldn't like to use it due to the fact that it can recuire much more domain knowledge which<br>\n",
    "we don't have. to make a desicion whether it is worth the hustle we would like to explore a little bit the dataset to get to know it better.<br>\n",
    "<br>\n",
    "Firstly we know each sample in the derived can be comprised out of several satelites<br>\n",
    "In the process of sampling the phone tries to communicate with as many GNSS system satelites as possible for maximum accurracy.<br>\n",
    "The more the merier.<br>\n",
    "Let's count the number of satelites that had been involved in the process.<br>\n",
    "We will calculate some metrics such as the total number of samples, the mean/std/max/percentiles number of satelites per sample.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvd_idx = derived.set_index(['collectionName','phoneName', 'millisSinceGpsEpoch'])\n",
    "print(\"The number of different satelites is: {}\".format(derived['svid'].nunique()))\n",
    "g = derived.groupby(['collectionName','phoneName', 'millisSinceGpsEpoch']).agg({'svid': [ len,lambda x: np.bincount(x).argmax()]})\n",
    "g.rename({'<lambda_0>': 'frequency', 'len': 'numOfSamples'}, axis=1, inplace=True)\n",
    "bnc = np.bincount(g[('svid', 'frequency')])\n",
    "print(\"The most frequent satelite is: {} with {} occurrences\".format(bnc.argmax(), max(bnc)))\n",
    "g.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed a single sample in a collection can be made out of as many as 109 datapoints. \n",
    "There are only 37 satelites, so we need to find out the source for the multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = g[g['svid', 'numOfSamples']==g['svid','numOfSamples'].max()]\n",
    "s2 = drvd_idx[drvd_idx['svid']==s.iloc[0]['svid','frequency']].loc[s.index[0]]\n",
    "s2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the fields are present as mentioned in the data section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From index column it is clear that we are currently inspecting 12 datapoints from single satelite at specific sample.<br>\n",
    "The unique table doesn't give out an immidiate suspect with 12 unique values, it is probably a combination of columns. <br>\n",
    "Lets try constellationType + signalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2['constellationSignal'] = s2['constellationType'].astype(str) + '_'+ s2['signalType']\n",
    "s2['constellationSignal'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 combinations for this sample therefore the  constellationType + signalType combination is not to blame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2.groupby(s2.columns.tolist(),as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple datapoints with duplicate rows, but still there are 8 different rows after grouping by all columns to find the pure duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s2.groupby(['rawPrM', 'receivedSvTimeInGpsNanos', 'constellationSignal'],as_index=False).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements are recieved at different timestamps and therefore the raw pseodo range is affected because of the movement of the satelite.<br>\n",
    "After dropping duplicates we still going to remain with several measurements per svid on specific sample due to multiple polling at different recieved times.<br>\n",
    "Before handeling the data we should drop the duplicate rows containing exactly the same values at every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_derived_count = derived.shape[0]\n",
    "derived.drop_duplicates(inplace=True)\n",
    "print(\"Dropped {} duplicate measurements\".format(old_derived_count-derived.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single sample 3d visualization\n",
    "for single android measurment visualize the specific point on earth the phone exists and the connected satelites,<br>\n",
    "Exibit the velocity of every satelite and the distance from the phone measured as the corrected pseudo range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSatelite(ax, x, phone_cor=None):\n",
    "    \"\"\"Plot satelite measurements for a specific sample.\n",
    "    \"\"\"\n",
    "    ax.scatter(x['xSatPosM'], x['ySatPosM'], x['zSatPosM'], c='r',s=50)\n",
    "    ax.quiver(x['xSatPosM'], x['ySatPosM'], x['zSatPosM'], x['xSatVelMps'], x['ySatVelMps'], x['zSatVelMps'], length=(x['xSatVelMps']**2+x['ySatVelMps']**2+x['zSatVelMps']**2)**0.5)\n",
    "    if isinstance(x.name, (int, np.integer)):\n",
    "        ax.text(x['xSatPosM'], x['ySatPosM'], x['zSatPosM']+10, str(x.name))\n",
    "    if phone_cor is not None:\n",
    "        ax.plot([x['xSatPosM'], phone_cor[0]], [x['ySatPosM'], phone_cor[1]], [x['zSatPosM'], phone_cor[2]], c='g')\n",
    "        \n",
    "def plotSphere(ax, r=6731000, center=(0,0,0), hRange=(0, 2 * np.pi), vRange=(0, np.pi), phoneLat=0, phoneLng=0):\n",
    "    theta = np.array([np.linspace(hRange[0], hRange[1], 50)])\n",
    "    theta = np.ones_like(theta).T @ theta\n",
    "    phi = np.array([np.linspace(vRange[0], vRange[1], 50)])\n",
    "    phi = np.ones_like(phi).T @ phi\n",
    "    phi = phi.T\n",
    "    \n",
    "    xx = r * np.sin(phi) * np.cos(theta) + center[0]\n",
    "    yy = r * np.sin(phi) * np.sin(theta) + center[1]\n",
    "    zz = r * np.cos(phi) + center[2]\n",
    "    \n",
    "    ax.plot_surface(xx, yy,zz)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def latlonToXYZ(lat, lon):\n",
    "    r=6731000 # Earth radius\n",
    "    return (float(r * np.sin(lat) * np.cos(lon)), float(r * np.sin(lat) * np.sin(lon)), float(r * np.cos(lat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived.reset_index(inplace=True)\n",
    "ground.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index of every sample is attributed to the collection, the phone used in the collection set <br>(The data is collected using multiple android phones in driving car)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived['millisSinceGpsEpoch_drvd'] = derived['millisSinceGpsEpoch']\n",
    "derived_idx = derived.set_index(['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose random sample from derived data to display satelites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot tries to sums up a single measurement in the derived dataset.<br>\n",
    "The blue sphere represent the earth.<br>\n",
    "The pink dot represents the north pole.<br>\n",
    "The green dot represents the android device making the measurement.<br>\n",
    "Each red dot represents a satelite in space.<br>\n",
    "Each satelite has vector representing its speed.<br>\n",
    "Each satelite streches a line in green which represents the distance to the the android device.<br>\n",
    "All the locations are appearing in ECEF coordinated system, with earth fixed in the center (the axis are the ECEF coordinated system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "plotSphere(ax)\n",
    "north_pole = latlonToXYZ(90, 0)\n",
    "ax.scatter(north_pole[0],north_pole[1],north_pole[2], c='pink', s=100)\n",
    "\n",
    "sample = derived[cpm].sample()\n",
    "while not np.any(ground['millisSinceGpsEpoch']==sample.values[0,2]):\n",
    "    sample = derived[cpm].sample()\n",
    "sample_target = ground[ground['millisSinceGpsEpoch']==sample.values[0,2]]\n",
    "\n",
    "plt.title(\"collectionName: {}  || Phone: {} ||  timestamp: {} ms\".format(sample.values[0,0], sample.values[0,1], sample.values[0,2]))\n",
    "satelites = derived_idx.loc[sample.values[0,0], sample.values[0,1], sample.values[0,2]]\n",
    "phone_cor = latlonToXYZ(sample_target['latDeg'], sample_target['lngDeg'])\n",
    "satelites.apply(lambda sat: plotSatelite(ax, sat, phone_cor), axis=1)\n",
    "ax.scatter(phone_cor[0],phone_cor[1],phone_cor[2], c='g', s=100)\n",
    "\n",
    "ax.text(phone_cor[0]+1000,phone_cor[1]+1000,phone_cor[2]+1000, \"lat:{}, lng:{}\".format(float(sample_target['latDeg'],), float(sample_target['lngDeg'])))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a specific collection and plot the collection's connection with different satelites.<br>\n",
    "The plot would be consisted of several line plots one for each satelite, on the x axis the epoch,<br>\n",
    "on the y axis the pseodo range (the distance from the satelite and the phone device) and we will try and correlate this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = derived[['collectionName', 'phoneName']].sample()\n",
    "df = derived.set_index(cpm).loc[sample.values[0,0], sample.values[0,1]]\n",
    "df.reset_index(inplace=True)\n",
    "df['rawPrM_norm'] = (df['rawPrM']-df['rawPrM'].min())/(df['rawPrM'].max()-df['rawPrM'].min())\n",
    "df['millisSinceGpsEpoch'] = (df['millisSinceGpsEpoch']-df['millisSinceGpsEpoch'].min())/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.scatterplot(data=df.sort_values('millisSinceGpsEpoch'), x='millisSinceGpsEpoch', y='rawPrM', hue='svid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth vs. derived `millisSinceGpsEpoch` comparison\n",
    "In order to further invistigate the relations between the derived data and the ground truth we will need to make sure these two datasets are synchronized.<br>\n",
    "The following code will check whether the number of epochs is the derived dataset is correlated with the no. of epochs at ground truth dataset.<br>\n",
    "Next we will check if the two datasets are aligned with each other.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if derived['millisSinceGpsEpoch'].nunique() == ground['millisSinceGpsEpoch'].nunique():\n",
    "    print(\"Derived and ground truth data has the same number of time ephocs\")\n",
    "    if np.all(derived['millisSinceGpsEpoch'] == ground['millisSinceGpsEpoch']):\n",
    "        print(\"Derived and ground truth data are synchronized\")\n",
    "    else:\n",
    "        print(\"Derived and ground truth data are not synchronized\")\n",
    "else:\n",
    "    print(\"Derived and ground truth data has different number of time ephocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The baseline and derived are correlated. moreover baseline is genrated using derived dataset\n",
    "* It seems that unfortunatly there is a difference between the timestamp of the samples in the ground truth\n",
    "  and the baseline as shown in the following. there are some samples that are the same regarding the timestamp and others which not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at how the number of samples are distributed over the ground truth dataset determined by the different collections.<br>\n",
    "Then we will try and compare it to the derived data set distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,15))\n",
    "ax = fig.gca()\n",
    "sns.countplot(data=ground, y='collectionName', hue='phoneName', ax=ax, palette='rocket')\n",
    "ax.legend(loc='upper right')\n",
    "ax.yaxis.grid(True)\n",
    "plt.title('Ground truth number of samples per collection')\n",
    "fig = plt.figure(figsize=(8,15))\n",
    "ax = fig.gca()\n",
    "sns.countplot(data=derived, y='collectionName', hue='phoneName', ax=ax, palette='rocket')\n",
    "ax.legend(loc='upper right')\n",
    "ax.yaxis.grid(True)\n",
    "plt.title('Derived number of samples per collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chart we can see the distribution of number of rows at each collection over the dataset split by phoneName.<br>\n",
    "On the upper chart we displayed the ground truth no. of samples and on the botton is the derived data.<br>\n",
    "The horizontal axis represent the number of rows in the dataframes, the vertical axis is the collection name and the color represent the phone.<br>\n",
    "We can notice right away few interesting things.<br>\n",
    "<br>\n",
    "Firstly derived collections contains a lot more data than the ground truth, which got us to think on the need to aggregate in the proper manner the measurements in order to archieve our target.<br>\n",
    "The reasonably look at the same proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will setup the constellation type mapping from the metadata for ease of chart reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "constellations = pd.read_csv('./google-smartphone-decimeter-challenge/metadata/constellation_type_mapping.csv')\n",
    "derived = derived.merge(constellations, on='constellationType', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart would display several features  frequency disributions over pie charts.<br>\n",
    "We chose to display the disribution of the phoneName, the signal type that has been recieved and the constellationType of the recieved satelite signals.<br>\n",
    "The mobile device type distribution over test and train collections is identical as is aserted and displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pieplot_on_columns(data, columns):\n",
    "    for c in columns:\n",
    "        data_dist = data.groupby(c).size().to_frame('size')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.title.set_text(f'{c} distribution by # of samples')\n",
    "        ax.pie(data_dist['size'], labels=list(data_dist.index), autopct=lambda x: f'{int(x)}%')\n",
    "        plt.show()\n",
    "\n",
    "pieplot_on_columns(derived, ['phoneName', 'signalType', 'constellationName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The vast majority of collections had been done using google's pixel4 phone.\n",
    "2. As expected the dominant constilationType is GPS in the US but the majority of samples actually come from non US systems such as GALILEO (europe), GLONASS(Russia),<br>\n",
    "Beidu and QZSS has small presence due to the fact that they are local systems that operates only in east asia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we are displaying a heatmap of the number of samples per satelite for each collection<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = derived[['collectionName', 'svid', 'millisSinceGpsEpoch']].groupby(['collectionName', 'svid']).count()\n",
    "d.reset_index(inplace=True)\n",
    "d = d.pivot_table(columns='svid', values='millisSinceGpsEpoch', index='collectionName', fill_value=0)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.gca()\n",
    "sns.heatmap(data=d, ax=ax)\n",
    "plt.title(\"Collection to satelite number of samples heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertical axis is the name of the collection, the horizontal axis is the satelite id.<br>\n",
    "The brighter the square is the more frequent the satelite is in the specific collection.<br>\n",
    "We can notice the busier satelites are svid: 2, 27, 30.<br>\n",
    "But we cannot jump to conclusions from this chart due to the fact that not every collection has been executed for the same time duration.<br>\n",
    "the longer the collection is the more samples are and the more important it would look on the heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots display the distribution of the X/Y/Z positions & velocity in the ECEF axis.<br>\n",
    "the distribution of ionic sphere and tropo sphere delay in the samples.<br>\n",
    "Another interesting feature we are looking at is the distribution of raw pseodo range.<br>\n",
    "this represent the distribution of how far the satelites are from the android device.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot constellation types\n",
    "# plot signal types\n",
    "# understand if all derived samples (key samples contain each of which signals and constellations)\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(8, 8, forward=True)\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['xSatPosM', 'ySatPosM', 'zSatPosM']]), ax = axs[0][0])\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['xSatVelMps', 'ySatVelMps', 'zSatVelMps']]), ax = axs[0][1])\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(derived[['ionoDelayM', 'tropoDelayM']]), ax = axs[1][0])\n",
    "sns.boxplot(data=pd.melt(derived[['rawPrM']]), x='variable',  y='value', ax= axs[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance from the satelites stays mostly betwween 2.2e7 to 2.5e7 with few outliers to the upper bound.<br>\n",
    "therefore we probably will need to notice more subtle changes to predict accurratly the location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore how the GNSS derived data correlates to the ground truth via dimensionality reducation.\n",
    "We will apply dimentionality reduction for better vizualizing our features and how they behave in comparison to the target vairables.<br>\n",
    "Due to the multidimentionality of the target variables, we would try to reduce the dimension of the target variables and the heightAboveWgs84EllipsoidM to 1 dimension.<br>\n",
    "Second, we would reduce selected features from derived data to 1 dimension<br>\n",
    "Finally we would plot the data on 2d axis for vizualizing the data.<br>\n",
    "We will perform the dimensionality reduction using Principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for ground truth and derived data correlation we will try to merge the two into one dataframe.<br>\n",
    "the problem is that ground truth has less than half the amount of rows that derived dataset has.<br>\n",
    "another problem from a glance at the two frames is that derived sometimes has a deviation of 1 millisecond from<br>\n",
    "the ground truth sample time. We believe it is due to rounding error and therefore can be dissmissed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grnd = ground.set_index(cpm)\n",
    "drvd = derived.set_index(cpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvd.loc[('2021-04-29-US-MTV-1', 'SamsungS20Ultra')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grnd.loc[('2021-04-29-US-MTV-1', 'SamsungS20Ultra')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected for a simple collection there are more than x46 rows at derived and the epochs are mostly off by 1 millisecond.<br>\n",
    "to fix this problem when we use merge we will use more flexible merge function that will have a tolerence of 1 millisecond forward that 1303770562999 will be the same as 1303770563000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvdGrnd = pd.merge_asof(derived.sort_values('millisSinceGpsEpoch'), ground.sort_values('millisSinceGpsEpoch'), on='millisSinceGpsEpoch',\n",
    "              suffixes=['_drvd', '_grnd'], tolerance=1, by=['collectionName', 'phoneName'], direction='forward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much missing values exists in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvdGrnd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 rows only in the ground truth columns, probably due to bigger deviation in the millisSinceGpsEpoch.<br>\n",
    "due to the low rate of missing values we can simply discard them in order to calculate PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvdGrnd.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better results before performing PCA it is usually recommended to perform normalization on the data.<br>\n",
    "We will use sklearn StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(drvdGrnd[['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM', 'courseDegree', 'speedMps']])\n",
    "drvdGrnd_normed_y = scaler_y.transform(drvdGrnd[['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM', 'courseDegree', 'speedMps']])\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(drvdGrnd[['xSatPosM', 'ySatPosM', 'zSatPosM', 'xSatVelMps', 'ySatVelMps', 'zSatVelMps', 'ionoDelayM', 'tropoDelayM', 'rawPrM', 'rawPrUncM', 'isrbM']])\n",
    "drvdGrnd_normed_x = scaler_x.transform(drvdGrnd[['xSatPosM', 'ySatPosM', 'zSatPosM', 'xSatVelMps', 'ySatVelMps', 'zSatVelMps', 'ionoDelayM', 'tropoDelayM', 'rawPrM', 'rawPrUncM', 'isrbM']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_y = PCA(n_components=1) \n",
    "y_transformed = pca_y.fit_transform(drvdGrnd_normed_y) \n",
    "pca_x = PCA(n_components=1) \n",
    "x_transformed = pca_x.fit_transform(drvdGrnd_normed_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x=x_transformed, y = y_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on the horizontal axis the derived data principal component of 1 component is set.<br>\n",
    "on the vertical axis the ground truth data principal component of 1 component is set.<br>\n",
    "As you can see it is a big mesh of a plot with no actual value for us beside that it looks kind of squarish?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the derived data it seems that at least for us it would be very hard to archive some kind of location calculation function without any previous domain knowledge in this field<br>\n",
    "Therefore we will try to firstly test how can we perform on much more simpler dataset, baseline, that contains already aggregated features.<br>\n",
    "The baseline can actually be our guideline and our model will just incorperate multiple samples and take better predictions that make small but accurate deviation from the baseline.<br>\n",
    "Later on we can try to incorprate the derived data but due to the fact that we don't know how to calculate lat/lon data from this raw measurements<br>\n",
    "maybe it would be best to simply use neural network to calculate it for us.<br>\n",
    "\n",
    "First of all we must explore the baseline dataframe to know how get around with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & data preperation\n",
    "\n",
    "* Clean the data from unwanted values\n",
    "* Generate added value features that can benefit our module.\n",
    "* Prepare the data for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "((bsln_trn.isna().sum(axis=0)/len(bsln_trn))*100).plot.barh()\n",
    "plt.title(\"missing values %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because leapsecond almost does not exists in the dataset (more than 60% is missing) we will discard this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn.drop('LeapSecond', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values we will fill the following<br>\n",
    "UncalAccelXMps2 - We assume constant speed (in the collections the data revolved around the zero term)<br>\n",
    "UncalAccelYMps2 - We assume constant speed (in the collections the data revolved around the zero term)<br>\n",
    "UncalAccelZMps2 - Besides the freefall given by earth's gravity we assume constant speed (in the collections the data revolved around the zero term).<br>\n",
    "<br>\n",
    "As seen from the plots most of the datapoints are around zero and it is logical to assume so because we mostly drive horizontaly.<br>\n",
    "Therefore we will fill the UncalGyro[X/Y/Z]RadPerSec with 0 degrees.<br>\n",
    "<br>\n",
    "For uncalibrated readings from the magnetometer we will use fill with mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn['UncalAccelXMps2'].fillna(0, inplace=True)\n",
    "bsln_trn['UncalAccelYMps2'].fillna(0, inplace=True)\n",
    "bsln_trn['UncalAccelZMps2'].fillna(9.81, inplace=True)\n",
    "bsln_trn['UncalGyroXRadPerSec'].fillna(0, inplace=True)\n",
    "bsln_trn['UncalGyroYRadPerSec'].fillna(0, inplace=True)\n",
    "bsln_trn['UncalGyroZRadPerSec'].fillna(0, inplace=True)\n",
    "bsln_trn['UncalMagXMicroT'].fillna(bsln_trn['UncalMagXMicroT'].mean(), inplace=True)\n",
    "bsln_trn['UncalMagYMicroT'].fillna(bsln_trn['UncalMagYMicroT'].mean(), inplace=True)\n",
    "bsln_trn['UncalMagZMicroT'].fillna(bsln_trn['UncalMagZMicroT'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said previous we would like to smooth out the locations because the base assumption is that changes in the locations aren't very big and can be calculated.<br>\n",
    "offcourse it is not the case but the current locations are way too jitter for it to be a real sequence of movement therefore a smoothing algorithm should be applyed<br>\n",
    "we chose kalman filter because it is a known practice in the navigation industry<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman filter\n",
    "The kalman filter is a method to smooth estimated measurements using its noise.<br>\n",
    "The filter works iterativly on timeseries data, at each epoch the `Kalman Gain` is calculated `KG = ERR_EST / (ERR_EST + ERR_MEA)`,<br> whereas `ERR_EST` represent the error in the estimation and `ERR_MEA` represent the error in the measurement.<br>\n",
    "Then the new estimated position is calculated using The following equation `EST_t-1 + KG(EST_t - EST_t-1)` <br>\n",
    "then the new error in the estimation is calculated using `(1-KG)ERR_MEA`.<br>\n",
    "This set of iterativly equations will determine the weight to give to each estimation over time and will smooth out the estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phone_col = 'phonePath'\n",
    "bsln_trn[phone_col] = bsln_trn['collectionName'] + bsln_trn['phoneName']\n",
    "lat_col = 'latDeg'\n",
    "lon_col = 'lngDeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_col = 'phonePath'\n",
    "bsln_tst[phone_col] = bsln_tst['collectionName'] + bsln_tst['phoneName']\n",
    "lat_col = 'latDeg'\n",
    "lon_col = 'lngDeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kf):\n",
    "    unique_paths = df[phone_col].unique()\n",
    "    for phone in tqdm(unique_paths):\n",
    "        data = df.loc[df[phone_col] == phone][[lat_col, lon_col]].values\n",
    "        data = data.reshape(1, len(data), 2)\n",
    "        smoothed = kf_.smooth(data)\n",
    "        df.loc[df[phone_col] == phone, lat_col] = smoothed.states.mean[0, :, 0]\n",
    "        df.loc[df[phone_col] == phone, lon_col] = smoothed.states.mean[0, :, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_tst_orig = bsln_tst.copy()\n",
    "bsln_tst_sm = apply_kf_smoothing(bsln_tst)\n",
    "bsln_tst_sm['latDeg_bsln_k'] = bsln_tst_sm['latDeg']\n",
    "bsln_tst_sm['lngDeg_bsln_k'] = bsln_tst_sm['lngDeg']\n",
    "#target = target.merge(bsln_trn_sm[['latDeg_bsln_k', 'lngDeg_bsln_k', 'collectionName', 'phoneName', 'millisSinceGpsEpoch']], on=cpm, how='left')\n",
    "#target['bsln_grnd_kalman'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln_k'], target['lngDeg_bsln_k'])\n",
    "#target[['bsln_grnd_kalman', 'bsln_grnd_distance']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn_orig = bsln_trn.copy()\n",
    "bsln_trn_sm = apply_kf_smoothing(bsln_trn)\n",
    "bsln_trn_sm['latDeg_bsln_k'] = bsln_trn_sm['latDeg']\n",
    "bsln_trn_sm['lngDeg_bsln_k'] = bsln_trn_sm['lngDeg']\n",
    "target = target.merge(bsln_trn_sm[['latDeg_bsln_k', 'lngDeg_bsln_k', 'collectionName', 'phoneName', 'millisSinceGpsEpoch']], on=cpm, how='left')\n",
    "target['bsln_grnd_kalman'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln_k'], target['lngDeg_bsln_k'])\n",
    "target[['bsln_grnd_kalman', 'bsln_grnd_distance']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things we notice right away is that the standard diviation decreased significantly by half.<br>\n",
    "Meaning our data is now much less noisy and jumpy, <br>\n",
    "The maximum value had reduced it's error to by more than 70% while the minimum value had been raised quite sharply by 600%.<br>\n",
    "All the other metrics had been benifited from the kalman smoothing which decreased the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'], label='raw estimates', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman'], label='kalman smoothed', alpha=0.7); \n",
    "plt.title('Smoothed baseline estimates using kalman filter'); \n",
    "plt.xlabel('no. sample'); \n",
    "plt.ylabel('horizontal distance (m)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some of our highst peaks in the error distance had been lowered.<br>\n",
    "Overall nice smooth action which flatted out some jittery predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_curr = haversine_50thP_95thP_mean(target['latDeg_grnd'], target['lngDeg_grnd'], target['latDeg_bsln_k'], target['lngDeg_bsln_k'])\n",
    "print(\"{}m error rate from our evaluation metric.\\n an impovement of {}m\".format(score_curr, score_prev-score_curr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement of 0.77m simply by using kalman filter with basic hyperparameters.<br>\n",
    "In the future we would like to tune the kalman filter hyperparameters for better results and even incorprate measurements from other devices like accelometer and gyro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean latDeg & lngDeg in baseline dataset over phones at the same epoch\n",
    "Each collection is made by driving car using one or more androind deviced to collect GNSS logs.<br>\n",
    "At each epoch several devices can collect GNSS data. Each of them should have the same lat/lng position<br>\n",
    "Therefore we would average over the devices the lat lng degrees.<br>\n",
    "As we can see not all the phones in our collections are synchronyzed.<br>\n",
    "Therefore we would like to mean the bucket of quarter of the seconds of epochs to leave some space for unsynchronized phones to average themeself.<br>\n",
    "The choice of 250 milliseconds was made by trial and error.<br>\n",
    "When averaging the prediction we use the assumption that the speed between each 250ms bucket is at max 65 mph (California highway speed limit) which between each bucket leave room for 3.6m error after averaging. most of the driving in the dataset is <br>made within the city, with a presumably much lower average speed so the error is much less critical.<br> Further invistagation regarding the speed at which the samples where taken will be introduced later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn = bsln_trn_sm.copy()\n",
    "bsln_trn.sort_values(cpm, inplace=True)\n",
    "bsln_trn['qsSinceGpsEpoch'] = bsln_trn.millisSinceGpsEpoch//250\n",
    "bsln_trn['millisSinceFirstEpoch'] = bsln_trn.millisSinceGpsEpoch - min(bsln_trn.millisSinceGpsEpoch)\n",
    "\n",
    "df = bsln_trn.groupby(['collectionName', 'qsSinceGpsEpoch']).agg({'latDeg': [ np.mean ], 'lngDeg': [ np.mean ], 'phoneName': [list], 'millisSinceFirstEpoch': [list]})\n",
    "bsln_mean_smoothed = pd.merge(bsln_trn, df.reset_index(), how='left', on=['collectionName', 'qsSinceGpsEpoch'], suffixes=('raw', 'mean'))\n",
    "score_prev = score_curr\n",
    "score_curr = haversine_50thP_95thP_mean(target['latDeg_grnd'], target['lngDeg_grnd'], bsln_mean_smoothed[('latDeg','mean')], bsln_mean_smoothed[('lngDeg','mean')])\n",
    "print(\"{}m error rate from our evaluation metric.\\n an impovement of {}m\".format(score_curr, score_prev-score_curr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement of 2.7 decimeters, not much but still not bad for simply avrage the different phones measurements over one sample.<br> It gets us to an error of 5.21m even before applying any ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target['bsln_grnd_kalman_mean_smooth'] = calc_haversine(target['latDeg_grnd'], target['lngDeg_grnd'], bsln_mean_smoothed[('latDeg','mean')], bsln_mean_smoothed[('lngDeg','mean')])\n",
    "target[['bsln_grnd_kalman_mean_smooth', 'bsln_grnd_kalman', 'bsln_grnd_distance']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics look a bit better for our smoothed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_distance'], label='raw estimates', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman'], label='kalman smoothed', alpha=0.7);\n",
    "plt.plot(range(target.shape[0]), target['bsln_grnd_kalman_mean_smooth'], label='kalman + mean smoothed', alpha=0.7);\n",
    "plt.title('Smoothed baseline estimates using kalman filter + mean phone average'); \n",
    "plt.xlabel('no. sample'); \n",
    "plt.ylabel('horizontal distance (m)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some smaller peaks had been smoothed. some had been lowered down, while some more accurate measurements had been pulled upwards<br>\n",
    "Overall it seems much better than the raw baseline measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignCommonColumns(left, right, columns={'latDeg': 'latDeg', 'lngDeg': 'lngDeg'}, on=cpm):\n",
    "    \"\"\"\n",
    "    The following is a helper procedure for assignment of columns\n",
    "    with keeping the rows aligned.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    left - left dataframe used in merge\n",
    "    right - right dataframe used in merge\n",
    "    columns - columns to assign to the left from the right, must exist on both dataframes\n",
    "        defaults to {'latDeg': 'latDeg', 'lngDeg': 'lngDeg'}\n",
    "    on - common columns that indicate specific datapoints for merge\n",
    "        defaults to ['collectionName', 'phoneName', 'millisSinceGpsEpoch']\n",
    "    \"\"\"\n",
    "    right_columns = list(set(on).union(set(columns.values())))\n",
    "    df = pd.merge(left, right[right_columns], how='inner', on=on)\n",
    "    for column in columns.keys():\n",
    "        if columns[column] == column:\n",
    "            df[column] = df[column+'_y']\n",
    "            df.drop([column+'_x',column+'_y'],axis=1,inplace=True)\n",
    "        else:\n",
    "            df[column] = df[columns[column]]\n",
    "            df.drop(columns[column],axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = assignCommonColumns(target, bsln_mean_smoothed, columns={'latDeg_bsln_k_mean': ('latDeg','mean'), 'lngDeg_bsln_k_mean': ('lngDeg','mean')})\n",
    "bsln_trn = assignCommonColumns(bsln_trn, bsln_mean_smoothed, columns={'latDeg': ('latDeg','mean'), 'lngDeg': ('lngDeg','mean')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(truth, data, suffixes=('',''), latcol = 'latDeg', lngcol='lngDeg'):\n",
    "    \"\"\"\n",
    "    This function is set to evaluate the haversine_50thP_95thP_mean of two datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    truth - ground truth dataframe\n",
    "    data - predictions dataframe\n",
    "    latcol - latitude degrees column name in the datasets. defaults to 'latDeg'\n",
    "    latcol - longitude degrees column name in the datasets. defaults to 'lngDeg'\n",
    "    suffixes - suffixes for the truth dataset and data dataset.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    the haversine_50thP_95thP_mean of the two datasets\n",
    "    \"\"\"\n",
    "    truthLat=latcol+suffixes[0]\n",
    "    truthLng=latcol+suffixes[1]\n",
    "    dataLat=lngcol+suffixes[0]\n",
    "    dataLng=lngcol+suffixes[1]\n",
    "    \n",
    "    #df = assignCommonColumns(truth, data, columns={ latcol+suffixes[0]: latcol+suffixes[1], lngcol+suffixes[0]: lngcol+suffixes[1]})\n",
    "    return haversine_50thP_95thP_mean(np.array(truth[latcol+suffixes[0]]), \n",
    "                                      np.array(truth[lngcol+suffixes[0]]), \n",
    "                                      np.array(data[latcol+suffixes[1]]), \n",
    "                                      np.array(data[lngcol+suffixes[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isSynched(bsln_trn, ground):\n",
    "    print(\"score: {}\".format(evaluate(bsln_trn, ground)))\n",
    "else:\n",
    "    print(\"Baseline and ground are somehow unsynched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#target['latDeg_bsln'] = bsln_trn['latDeg']\n",
    "#target['lngDeg_bsln'] = bsln_trn['lngDeg']\n",
    "assignCommonColumns(target, bsln_trn, columns={'latDeg_bsln': 'latDeg', 'lngDeg_bsln': 'lngDeg'})\n",
    "drawRandomPath(collection='2021-04-29-US-MTV-1', phone='Pixel4', mapName='2021-04-29-US-MTV-1-Pixel4-Smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to our previous plot this heat map look much concentraited and much better as a path of movement<br>\n",
    "You can see this along the turns, the points are managing to stay somewhat together and less spreadout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate corrected pseudo range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite) can be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived['correctedPrM'] = derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - derived['ionoDelayM'] -derived['tropoDelayM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Foreach sample take the previous location of the phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As timeseries nature of the collection it is expected to give our previous estimation weight in our current estimation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn['latDeg_prv'] = bsln_trn['latDeg']\n",
    "bsln_trn['lngDeg_prv'] = bsln_trn['lngDeg']\n",
    "bsln_trn.loc[bsln_trn['collectionName']==bsln_trn.shift()['collectionName'], 'latDeg_prv'] = bsln_trn.shift()['latDeg']\n",
    "bsln_trn.loc[bsln_trn['collectionName']==bsln_trn.shift()['collectionName'], 'latDeg_prv'] = bsln_trn.shift()['lngDeg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add datetime timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of analysis create timestamp columns which are much more human readable than `millisSinceGpsEpoch`<br>\n",
    "note - `millisSinceGpsEpoch` is the milliseconds passed since 6th Jan 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTime = datetime.datetime(1980,1,6,0,0,0,0)\n",
    "derived['epoch_timestamp'] = derived['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))\n",
    "ground['epoch_timestamp'] = ground['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))\n",
    "bsln_trn['epoch_timestamp'] = bsln_trn['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of the derived dataset we will narrow it down to collection-phone-epoch samples which each samples contains the relative <br>\n",
    "information regarding each satelite in the sample. If the satelite is not appearing in the current measurement all it's fields would <br>\n",
    "There are usually more than 1 sample per specific `collectionName`-`phoneName`-`millisSinceGpsEpoch`-`svid` therefore we will<br> groupby this key and use mean on our numric features before using pivot table.<br>\n",
    "correspondedly be equal to NaN. We cannot simply use 0 as our distance from them and wrap it up with `fillna(0)` <br>\n",
    "We would have to find a way to make our model ignore these values, because there is no such thing as minus distance from earth <br>\n",
    "We will assign NaN distances with `-max(correctedPrM)` to make sure our module is encourged to diffrentiate it from the true measurements and ignore it<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = {'correctedPrM', 'xSatPosM', 'ySatPosM', 'zSatPosM', 'constellationType', 'xSatVelMps', 'ySatVelMps', 'zSatVelMps', 'satClkBiasM', 'satClkDriftMps'  }\n",
    "df = derived[list(features.union({'collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid'}))].groupby(['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'svid']).mean()\n",
    "piv = df.pivot_table(values=list(features), index=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], columns=['svid'])\n",
    "grouped = derived[['phoneName','collectionName', 'millisSinceGpsEpoch', 'receivedSvTimeInGpsNanos', 'epoch_timestamp']].groupby(['phoneName', 'millisSinceGpsEpoch']).max()\n",
    "piv = piv.merge(grouped, on=['phoneName', 'millisSinceGpsEpoch'])\n",
    "piv.fillna(-max(derived['correctedPrM']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and correlate the derived data to the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seen before, the number of derived epochs is different that the number of epoch is ground truth dataset. <br>\n",
    "This is a huge problem because we need to correlate the target data to the training data. <br>\n",
    "Our main objective is to predict accurratly the lat/lng position of the phone in the world using the derived data and baseline data.\n",
    "<br>\n",
    "One obsticle in doing so is that the derived dataset grouped by time epochs does not necesseraly own the same time epochs as baseline.<br>\n",
    "This problem reoccurres again when trying to compare to the target data.<br>\n",
    "So before we even do any training we need to figure out the cause for this incosistency. Or at least when does it occurres and at which scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsln_trn['epoch_timestamp'] = bsln_trn['millisSinceGpsEpoch'].apply(lambda x: datetime.datetime.fromtimestamp(baseTime.timestamp()+x/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln = bsln_trn.set_index('millisSinceGpsEpoch').sort_index()\n",
    "piv = piv.reset_index().set_index('millisSinceGpsEpoch').sort_index()\n",
    "df = pd.merge_asof(bsln, piv, on='millisSinceGpsEpoch',by='phoneName', suffixes=('_bsln', '_piv'), direction='nearest', tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of rows that are outside the tolerance range are {}.\\nThere are total {} samples\".format(df['epoch_timestamp_piv'].isna().sum(), df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn.shape[0]-piv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{}% of the baseline is missing from derived\".format((df['epoch_timestamp_piv'].isna().sum()/df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed before the derived data has 1003 missing records from baseline.<br>\n",
    "Therefore we can be satisfied with 1013 missing which is a very small percentage of the data.<br>\n",
    "nethertheless it is still part of the data and we should decide what to do next with our null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(df, ground[['phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], how='inner', left_on=['phoneName', 'millisSinceGpsEpoch'], right_on=['phoneName','millisSinceGpsEpoch'], suffixes=('_bsln', '_grnd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "# Model Training\n",
    "## Prepare data for training\n",
    "Firstly we want to make sure our data is synched with ground truth and we are predicting the correct samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isSynched(bsln_trn, ground):\n",
    "    bsln_trn.sort_values(cpm, axis=1)\n",
    "    ground.sort_values(cpm, axis=1)\n",
    "    assert isSynched(bsln_trn, ground), 'baseline and ground data are not aligned'\n",
    "        \n",
    "features = ['collectionName', 'phoneName', 'latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM', 'UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2', 'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec','UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT']\n",
    "target_var = ['collectionName', 'phoneName','latDeg', 'lngDeg']\n",
    "X = bsln_trn[features]\n",
    "#X.set_index('collectionName', inplace=True)\n",
    "#X = pd.get_dummies(X)\n",
    "y = ground[target_var]\n",
    "#y.set_index('collectionName', inplace=True)\n",
    "\n",
    "train = X.copy()\n",
    "labels = y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the error correction nature of the problem, initially we would like to see how a ML model can reproduce the simple baseline prediction<br>\n",
    "using only its baseline latlon and even try and make it better by using timeseries model such as LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for train/test the data & normilize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the model to perform better on the data we will normalize our data before feeding it to the model.<br>\n",
    "We chose RobustScaler because it is robust to outliers. our data contains many outliers therfore it is worth<br>\n",
    "using this type of scaler. This Scaler removes the median and scales the data according to the quantile range\n",
    "\n",
    "Then we split the normalized dataset to train and validation sets by 80%/20%.<br>\n",
    "the split would be preformed as such that whole collections won't be seperated because of the sequence nature of our data.<br>\n",
    "roughly 80% of the collections will be assigned for training while 20% would be assigned for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized  train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, labels, random_state=0, test_size=0.2)\n",
    "scaler_x_trn = RobustScaler()\n",
    "scaler_y_trn = RobustScaler()\n",
    "scaler_x_val = RobustScaler()\n",
    "scaler_y_val = RobustScaler()\n",
    "\n",
    "x_train_norm = scaler_x_trn.fit_transform(x_train)\n",
    "y_train_norm = scaler_y_trn.fit_transform(y_train)\n",
    "x_val_norm = scaler_x_val.fit_transform(x_val)\n",
    "y_val_norm = scaler_y_val.fit_transform(y_val)\n",
    "\n",
    "print(\"Baseline train loss is currently stands at:{}m\".format(evaluate(y_train, x_train)))\n",
    "print(\"\\nBaseline validation loss is currently stands at:{}m\".format(evaluate(y_val, x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split by collections and without randomizing the data (will be used later for sequence models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly we will generate a scaler and fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobustScaler()"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "scaler_x.fit(train.drop(['collectionName', 'phoneName'], axis=1))\n",
    "scaler_y.fit(labels.drop(['collectionName', 'phoneName'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train.shape[1]\n",
    "\n",
    "train.set_index('collectionName', inplace=True)\n",
    "labels.set_index('collectionName', inplace=True)\n",
    "# Split to train and validation\n",
    "n_collections =len(train.index.unique())\n",
    "n_train_collections = math.ceil(0.8*n_collections)\n",
    "train_collections = train.index.unique()[:n_train_collections]\n",
    "validation_collections = train.index.unique()[n_train_collections:]\n",
    "x_train_clc = train.loc[train_collections]\n",
    "y_train_clc = labels.loc[train_collections]\n",
    "x_val_clc   = train.loc[validation_collections]\n",
    "y_val_clc   = labels.loc[validation_collections]\n",
    "\n",
    "\n",
    "#scaler_x_trn_clc = RobustScaler()\n",
    "#scaler_y_trn_clc = RobustScaler()\n",
    "#scaler_x_val_clc = RobustScaler()\n",
    "#scaler_y_val_clc = RobustScaler()\n",
    "#\n",
    "#x_train_norm_clc = scaler_x_trn_clc.fit_transform(x_train_clc)\n",
    "#y_train_norm_clc = scaler_y_trn_clc.fit_transform(y_train_clc)\n",
    "#x_val_norm_clc = scaler_x_val_clc.fit_transform(x_val_clc)\n",
    "#y_val_norm_clc = scaler_y_val_clc.fit_transform(y_val_clc)\n",
    "#\n",
    "#print(\"Baseline train loss is currently stands at:{}m using the following collections: {}\"\\\n",
    "#      .format(evaluate(y_train_clc, x_train_clc), list(train_collections)))\n",
    "#print(\"\\nBaseline validation loss is currently stands at:{}m using the following collections: {}\"\\\n",
    "#      .format(evaluate(y_val_clc, x_val_clc), list(validation_collections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "train_seq = []\n",
    "val_seq = []\n",
    "\n",
    "n_future = 1   # Number of epochs we want to look into the future based on the past epochs.\n",
    "n_past = 30  # Number of past epochs we want to use to predict the future.\n",
    "\n",
    "collections = shuffle(train[['collectionName', 'phoneName']].drop_duplicates())\n",
    "train_path = shuffle(collections.set_index('collectionName').loc[train_collections].reset_index())\n",
    "valid_path = shuffle(collections.set_index('collectionName').loc[validation_collections].reset_index())\n",
    "\n",
    "for pair in np.array(train_path):\n",
    "    x = x_train_clc.loc[pair[0]][x_train_clc.loc[pair[0]]['phoneName']==pair[1]].drop(['phoneName'], axis=1)\n",
    "    y = y_train_clc.loc[pair[0]][y_train_clc.loc[pair[0]]['phoneName']==pair[1]].drop(['phoneName'], axis=1)\n",
    "    x = scaler_x.transform(x)\n",
    "    y = scaler_y.transform(y)\n",
    "    train_seq.append(tf.keras.preprocessing.sequence.TimeseriesGenerator(x,y,30, batch_size=y.shape[0]))\n",
    "    \n",
    "for pair in np.array(valid_path):\n",
    "    x = x_val_clc.loc[pair[0]][x_val_clc.loc[pair[0]]['phoneName']==pair[1]].drop(['phoneName'], axis=1)\n",
    "    y = y_val_clc.loc[pair[0]][y_val_clc.loc[pair[0]]['phoneName']==pair[1]].drop(['phoneName'], axis=1)\n",
    "    x = scaler_x.transform(x)\n",
    "    y = scaler_y.transform(y)\n",
    "    val_seq.append(tf.keras.preprocessing.sequence.TimeseriesGenerator(x,y,30, batch_size=y.shape[0]))\n",
    "    \n",
    "trainX = train_seq[0][0][0]\n",
    "trainY = train_seq[0][0][1]\n",
    "\n",
    "for i in range(1, len(train_seq)):\n",
    "    trainX = np.vstack((trainX, train_seq[i][0][0]))\n",
    "    trainY = np.vstack((trainY, train_seq[i][0][1]))\n",
    "    \n",
    "valX = val_seq[0][0][0]\n",
    "valY = val_seq[0][0][1]\n",
    "\n",
    "for i in range(1, len(val_seq)):\n",
    "    valX = np.vstack((valX, val_seq[i][0][0]))\n",
    "    valY = np.vstack((valY, val_seq[i][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train loss is currently stands at:3.8699111193066495dm using the following collections: ['2020-05-14-US-MTV-1', '2020-05-14-US-MTV-2', '2020-05-21-US-MTV-1', '2020-05-21-US-MTV-2', '2020-05-29-US-MTV-1', '2020-05-29-US-MTV-2', '2020-06-04-US-MTV-1', '2020-06-05-US-MTV-1', '2020-06-05-US-MTV-2', '2020-06-11-US-MTV-1', '2020-07-08-US-MTV-1', '2020-07-17-US-MTV-1', '2020-07-17-US-MTV-2', '2020-08-03-US-MTV-1', '2020-08-06-US-MTV-2', '2020-09-04-US-SF-1', '2020-09-04-US-SF-2', '2021-01-04-US-RWC-1', '2021-01-04-US-RWC-2', '2021-01-05-US-SVL-1', '2021-01-05-US-SVL-2', '2021-03-10-US-SVL-1', '2021-04-15-US-MTV-1', '2021-04-22-US-SJC-1']\n",
      "\n",
      "Baseline validation loss is currently stands at:10.850101562328003dm using the following collections: ['2021-04-26-US-SVL-1', '2021-04-28-US-MTV-1', '2021-04-28-US-SJC-1', '2021-04-29-US-MTV-1', '2021-04-29-US-SJC-2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline train loss is currently stands at:{}dm using the following collections: {}\"\\\n",
    "      .format(evaluate(y_train_clc, x_train_clc), list(train_collections)))\n",
    "print(\"\\nBaseline validation loss is currently stands at:{}dm using the following collections: {}\"\\\n",
    "      .format(evaluate(y_val_clc, x_val_clc), list(validation_collections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train_seq[0][0][0]\n",
    "trainY = train_seq[0][0][1]\n",
    "\n",
    "for i in range(1, len(train_seq)):\n",
    "    trainX = np.vstack((trainX, train_seq[i][0][0]))\n",
    "    trainY = np.vstack((trainY, train_seq[i][0][1]))\n",
    "    \n",
    "valX = val_seq[0][0][0]\n",
    "valY = val_seq[0][0][1]\n",
    "\n",
    "for i in range(1, len(val_seq)):\n",
    "    valX = np.vstack((valX, val_seq[i][0][0]))\n",
    "    valY = np.vstack((valY, val_seq[i][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21329, 30, 12)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4585, 2)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_gen[-1][0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we feed our network we need to reformat our data in sequences.<br>\n",
    "We need to select windows size and the predicted window range.<br>\n",
    "We will choose a the window to be made of a phone last 30 epochs in a collections, which means,<br>\n",
    "a look of 30 seconds into the past to predict the present.<br>\n",
    "To create the windows we will use built in keras preprocessing object TimeseriesGenerator which generates a data generator<br>\n",
    "instance. the data generator generates batches of sequences of 30 epochs that each are going through the network at it's<br>\n",
    "own turn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x_train_norm_clc, y_train_norm_clc, 30)\n",
    "val_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x_train_norm_clc, y_train_norm_clc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latDeg</th>\n",
       "      <th>lngDeg</th>\n",
       "      <th>heightAboveWgs84EllipsoidM</th>\n",
       "      <th>UncalAccelXMps2</th>\n",
       "      <th>UncalAccelYMps2</th>\n",
       "      <th>UncalAccelZMps2</th>\n",
       "      <th>UncalGyroXRadPerSec</th>\n",
       "      <th>UncalGyroYRadPerSec</th>\n",
       "      <th>UncalGyroZRadPerSec</th>\n",
       "      <th>UncalMagXMicroT</th>\n",
       "      <th>...</th>\n",
       "      <th>UncalMagZMicroT</th>\n",
       "      <th>latDeg_prv</th>\n",
       "      <th>lngDeg_prv</th>\n",
       "      <th>phoneName_Mi8</th>\n",
       "      <th>phoneName_Pixel4</th>\n",
       "      <th>phoneName_Pixel4Modded</th>\n",
       "      <th>phoneName_Pixel4XL</th>\n",
       "      <th>phoneName_Pixel4XLModded</th>\n",
       "      <th>phoneName_Pixel5</th>\n",
       "      <th>phoneName_SamsungS20Ultra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collectionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-05-14-US-MTV-1</th>\n",
       "      <td>37.423549</td>\n",
       "      <td>-122.094006</td>\n",
       "      <td>-34.06</td>\n",
       "      <td>0.715112</td>\n",
       "      <td>9.248114</td>\n",
       "      <td>1.379163</td>\n",
       "      <td>-0.007518</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>25.262530</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.042335</td>\n",
       "      <td>37.423549</td>\n",
       "      <td>-122.094006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14-US-MTV-1</th>\n",
       "      <td>37.423564</td>\n",
       "      <td>-122.094063</td>\n",
       "      <td>-33.29</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>10.207922</td>\n",
       "      <td>1.447936</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>-0.017295</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>25.519350</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.145015</td>\n",
       "      <td>-122.094006</td>\n",
       "      <td>-122.094063</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14-US-MTV-1</th>\n",
       "      <td>37.423573</td>\n",
       "      <td>-122.094098</td>\n",
       "      <td>-30.99</td>\n",
       "      <td>0.220563</td>\n",
       "      <td>9.922440</td>\n",
       "      <td>1.318199</td>\n",
       "      <td>-0.007424</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>25.460735</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.096955</td>\n",
       "      <td>-122.094063</td>\n",
       "      <td>-122.094098</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14-US-MTV-1</th>\n",
       "      <td>37.423563</td>\n",
       "      <td>-122.094086</td>\n",
       "      <td>-32.83</td>\n",
       "      <td>0.370482</td>\n",
       "      <td>9.662949</td>\n",
       "      <td>1.337634</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>-0.016661</td>\n",
       "      <td>25.715742</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.220626</td>\n",
       "      <td>-122.094098</td>\n",
       "      <td>-122.094086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14-US-MTV-1</th>\n",
       "      <td>37.423571</td>\n",
       "      <td>-122.094115</td>\n",
       "      <td>-34.49</td>\n",
       "      <td>-0.015159</td>\n",
       "      <td>9.870129</td>\n",
       "      <td>0.896537</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.012372</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>25.976639</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.371474</td>\n",
       "      <td>-122.094086</td>\n",
       "      <td>-122.094115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-22-US-SJC-1</th>\n",
       "      <td>37.334601</td>\n",
       "      <td>-121.899432</td>\n",
       "      <td>-6.66</td>\n",
       "      <td>-1.146823</td>\n",
       "      <td>9.466972</td>\n",
       "      <td>-1.089362</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.015272</td>\n",
       "      <td>1.260000</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.199999</td>\n",
       "      <td>-121.899432</td>\n",
       "      <td>-121.899432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-22-US-SJC-1</th>\n",
       "      <td>37.334607</td>\n",
       "      <td>-121.899427</td>\n",
       "      <td>-10.68</td>\n",
       "      <td>-1.134852</td>\n",
       "      <td>9.598654</td>\n",
       "      <td>-1.197101</td>\n",
       "      <td>0.014661</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>1.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.960000</td>\n",
       "      <td>-121.899432</td>\n",
       "      <td>-121.899427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-22-US-SJC-1</th>\n",
       "      <td>37.334612</td>\n",
       "      <td>-121.899421</td>\n",
       "      <td>-8.80</td>\n",
       "      <td>-1.204284</td>\n",
       "      <td>9.797373</td>\n",
       "      <td>-1.465251</td>\n",
       "      <td>-0.014661</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.800000</td>\n",
       "      <td>-121.899427</td>\n",
       "      <td>-121.899421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-22-US-SJC-1</th>\n",
       "      <td>37.334611</td>\n",
       "      <td>-121.899415</td>\n",
       "      <td>-6.98</td>\n",
       "      <td>-1.098939</td>\n",
       "      <td>9.395146</td>\n",
       "      <td>-1.041478</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.260000</td>\n",
       "      <td>-121.899421</td>\n",
       "      <td>-121.899415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-22-US-SJC-1</th>\n",
       "      <td>37.334604</td>\n",
       "      <td>-121.899410</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>-1.173159</td>\n",
       "      <td>9.466972</td>\n",
       "      <td>-1.244985</td>\n",
       "      <td>0.021380</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.016493</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.619999</td>\n",
       "      <td>-121.899415</td>\n",
       "      <td>-121.899410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109653 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        latDeg      lngDeg  heightAboveWgs84EllipsoidM  \\\n",
       "collectionName                                                           \n",
       "2020-05-14-US-MTV-1  37.423549 -122.094006                      -34.06   \n",
       "2020-05-14-US-MTV-1  37.423564 -122.094063                      -33.29   \n",
       "2020-05-14-US-MTV-1  37.423573 -122.094098                      -30.99   \n",
       "2020-05-14-US-MTV-1  37.423563 -122.094086                      -32.83   \n",
       "2020-05-14-US-MTV-1  37.423571 -122.094115                      -34.49   \n",
       "...                        ...         ...                         ...   \n",
       "2021-04-22-US-SJC-1  37.334601 -121.899432                       -6.66   \n",
       "2021-04-22-US-SJC-1  37.334607 -121.899427                      -10.68   \n",
       "2021-04-22-US-SJC-1  37.334612 -121.899421                       -8.80   \n",
       "2021-04-22-US-SJC-1  37.334611 -121.899415                       -6.98   \n",
       "2021-04-22-US-SJC-1  37.334604 -121.899410                       -6.54   \n",
       "\n",
       "                     UncalAccelXMps2  UncalAccelYMps2  UncalAccelZMps2  \\\n",
       "collectionName                                                           \n",
       "2020-05-14-US-MTV-1         0.715112         9.248114         1.379163   \n",
       "2020-05-14-US-MTV-1         0.005597        10.207922         1.447936   \n",
       "2020-05-14-US-MTV-1         0.220563         9.922440         1.318199   \n",
       "2020-05-14-US-MTV-1         0.370482         9.662949         1.337634   \n",
       "2020-05-14-US-MTV-1        -0.015159         9.870129         0.896537   \n",
       "...                              ...              ...              ...   \n",
       "2021-04-22-US-SJC-1        -1.146823         9.466972        -1.089362   \n",
       "2021-04-22-US-SJC-1        -1.134852         9.598654        -1.197101   \n",
       "2021-04-22-US-SJC-1        -1.204284         9.797373        -1.465251   \n",
       "2021-04-22-US-SJC-1        -1.098939         9.395146        -1.041478   \n",
       "2021-04-22-US-SJC-1        -1.173159         9.466972        -1.244985   \n",
       "\n",
       "                     UncalGyroXRadPerSec  UncalGyroYRadPerSec  \\\n",
       "collectionName                                                  \n",
       "2020-05-14-US-MTV-1            -0.007518             0.002428   \n",
       "2020-05-14-US-MTV-1             0.003152            -0.017295   \n",
       "2020-05-14-US-MTV-1            -0.007424             0.000062   \n",
       "2020-05-14-US-MTV-1             0.003219             0.002157   \n",
       "2020-05-14-US-MTV-1            -0.013343             0.012372   \n",
       "...                                  ...                  ...   \n",
       "2021-04-22-US-SJC-1            -0.006720             0.005498   \n",
       "2021-04-22-US-SJC-1             0.014661             0.006720   \n",
       "2021-04-22-US-SJC-1            -0.014661             0.007330   \n",
       "2021-04-22-US-SJC-1             0.006720             0.010996   \n",
       "2021-04-22-US-SJC-1             0.021380             0.007330   \n",
       "\n",
       "                     UncalGyroZRadPerSec  UncalMagXMicroT  ...  \\\n",
       "collectionName                                             ...   \n",
       "2020-05-14-US-MTV-1            -0.003135        25.262530  ...   \n",
       "2020-05-14-US-MTV-1             0.001801        25.519350  ...   \n",
       "2020-05-14-US-MTV-1             0.005446        25.460735  ...   \n",
       "2020-05-14-US-MTV-1            -0.016661        25.715742  ...   \n",
       "2020-05-14-US-MTV-1             0.001747        25.976639  ...   \n",
       "...                                  ...              ...  ...   \n",
       "2021-04-22-US-SJC-1             0.015272         1.260000  ...   \n",
       "2021-04-22-US-SJC-1             0.015882         1.920000  ...   \n",
       "2021-04-22-US-SJC-1             0.013439         0.540000  ...   \n",
       "2021-04-22-US-SJC-1             0.017715         1.560000  ...   \n",
       "2021-04-22-US-SJC-1             0.016493         1.380000  ...   \n",
       "\n",
       "                     UncalMagZMicroT  latDeg_prv  lngDeg_prv  phoneName_Mi8  \\\n",
       "collectionName                                                                \n",
       "2020-05-14-US-MTV-1       -14.042335   37.423549 -122.094006              0   \n",
       "2020-05-14-US-MTV-1       -14.145015 -122.094006 -122.094063              0   \n",
       "2020-05-14-US-MTV-1       -13.096955 -122.094063 -122.094098              0   \n",
       "2020-05-14-US-MTV-1       -13.220626 -122.094098 -122.094086              0   \n",
       "2020-05-14-US-MTV-1       -14.371474 -122.094086 -122.094115              0   \n",
       "...                              ...         ...         ...            ...   \n",
       "2021-04-22-US-SJC-1       -19.199999 -121.899432 -121.899432              0   \n",
       "2021-04-22-US-SJC-1       -18.960000 -121.899432 -121.899427              0   \n",
       "2021-04-22-US-SJC-1       -19.800000 -121.899427 -121.899421              0   \n",
       "2021-04-22-US-SJC-1       -19.260000 -121.899421 -121.899415              0   \n",
       "2021-04-22-US-SJC-1       -19.619999 -121.899415 -121.899410              0   \n",
       "\n",
       "                     phoneName_Pixel4  phoneName_Pixel4Modded  \\\n",
       "collectionName                                                  \n",
       "2020-05-14-US-MTV-1                 1                       0   \n",
       "2020-05-14-US-MTV-1                 1                       0   \n",
       "2020-05-14-US-MTV-1                 1                       0   \n",
       "2020-05-14-US-MTV-1                 1                       0   \n",
       "2020-05-14-US-MTV-1                 1                       0   \n",
       "...                               ...                     ...   \n",
       "2021-04-22-US-SJC-1                 0                       0   \n",
       "2021-04-22-US-SJC-1                 0                       0   \n",
       "2021-04-22-US-SJC-1                 0                       0   \n",
       "2021-04-22-US-SJC-1                 0                       0   \n",
       "2021-04-22-US-SJC-1                 0                       0   \n",
       "\n",
       "                     phoneName_Pixel4XL  phoneName_Pixel4XLModded  \\\n",
       "collectionName                                                      \n",
       "2020-05-14-US-MTV-1                   0                         0   \n",
       "2020-05-14-US-MTV-1                   0                         0   \n",
       "2020-05-14-US-MTV-1                   0                         0   \n",
       "2020-05-14-US-MTV-1                   0                         0   \n",
       "2020-05-14-US-MTV-1                   0                         0   \n",
       "...                                 ...                       ...   \n",
       "2021-04-22-US-SJC-1                   0                         0   \n",
       "2021-04-22-US-SJC-1                   0                         0   \n",
       "2021-04-22-US-SJC-1                   0                         0   \n",
       "2021-04-22-US-SJC-1                   0                         0   \n",
       "2021-04-22-US-SJC-1                   0                         0   \n",
       "\n",
       "                     phoneName_Pixel5  phoneName_SamsungS20Ultra  \n",
       "collectionName                                                    \n",
       "2020-05-14-US-MTV-1                 0                          0  \n",
       "2020-05-14-US-MTV-1                 0                          0  \n",
       "2020-05-14-US-MTV-1                 0                          0  \n",
       "2020-05-14-US-MTV-1                 0                          0  \n",
       "2020-05-14-US-MTV-1                 0                          0  \n",
       "...                               ...                        ...  \n",
       "2021-04-22-US-SJC-1                 0                          1  \n",
       "2021-04-22-US-SJC-1                 0                          1  \n",
       "2021-04-22-US-SJC-1                 0                          1  \n",
       "2021-04-22-US-SJC-1                 0                          1  \n",
       "2021-04-22-US-SJC-1                 0                          1  \n",
       "\n",
       "[109653 rows x 21 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 1   # Number of epochs we want to look into the future based on the past epochs.\n",
    "n_past = 30  # Number of past epochs we want to use to predict the future.\n",
    "\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(x_train_norm_clc) - n_future +1):\n",
    "    trainX.append(x_train_norm_clc[i - n_past:i])\n",
    "    trainY.append(y_train_norm_clc[i + n_future - 1:i + n_future])\n",
    "    \n",
    "trainX = np.array(trainX)\n",
    "trainY = np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))\n",
    "\n",
    "#Empty lists to be populated using formatted valing data\n",
    "valX = []\n",
    "valY = []\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_valing_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(x_val_norm_clc) - n_future +1):\n",
    "    valX.append(x_val_norm_clc[i - n_past:i])\n",
    "    valY.append(y_val_norm_clc[i + n_future - 1:i + n_future])\n",
    "    \n",
    "valX = np.array(valX)\n",
    "valY = np.array(valY)\n",
    "\n",
    "print('valX shape == {}.'.format(valX.shape))\n",
    "print('valY shape == {}.'.format(valY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Datasets model\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Learning - regression models </h2>\n",
    "\n",
    "Considering the continous nature of some of the features (speed, X/Y/Z coordinates, course degrees and etc.) it might be useful to try some regression models - perhaps those will</br>\n",
    "be able to capture the continous nature of the relation between the predicted lat/long values and the physcial properties features.</br>\n",
    "The input features considered for this model should be numeric, describing different satelite properties from the derived dataset combined with baseline estimations.<br>\n",
    "First, we think about using the already existing \"pivotted\" set - where derived data that is satellite specific is taken as a seperate feature for each satellite. Some example features:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "[c for c in list(data.columns) if 'correctedPrM' in c][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in thsi case, the corrected estimated distance from each satellite to a cell phone is considered a seperate feature.<br>\n",
    "Preparing for this kind of model, we drop categorial data and unneccesary textual and time fields, since we want to observe regression predicting based only on continous values for now.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_drop = ['constellationType', 'receivedSvTimeInGpsNanos']\n",
    "df = data.drop([c for c in list(data.columns) if any([cname in c for cname in c_drop])], axis=1)\n",
    "df = df.select_dtypes(include=['float64', 'int64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to utilize different regression models, we define 2 things:<br>\n",
    "- A scoring method, getting two arrays - prediction values and ground values. It will determine the closeness of the prediction set using the method mentioned in the challenge.<br>\n",
    "- We can guess that some code will repeat itself - splitting data, fitting, predicting, getting the mean score and etc...so to make things concise and to reduce the number of lines of code, we also define a method for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def haversine_medians_loss(pred, grnd):\n",
    "    return haversine_50thP_95thP_mean(pred[:,0], pred[:,1], grnd[:,0], grnd[:,1])\n",
    "\n",
    "def utilize_model(model, data, filter_on_train_data=None, predict_columns=['latDeg_grnd', 'lngDeg_grnd'], cv_folds=5):\n",
    "    \"\"\"\n",
    "    Train a model using cross validation, predict and return a score for the prediction based on ground values.\n",
    "    \n",
    "    :param model: A machine learning model object implementing *fit* and *predict* methods    \n",
    "    :param data: A pandas DataFrame object containing columns to train on and columns to predict.\n",
    "    :param predict_columns: an array-like object of columns strings to try and predict\n",
    "    :param cv_folds (optional): how many folds should be used when training and testing (cross-validating).\n",
    "    \"\"\"\n",
    "    Y = data[predict_columns].to_numpy()\n",
    "    X = data.drop(predict_columns, axis=1).to_numpy()\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True)\n",
    "    for train, test in kf.split(X):\n",
    "        model.fit(X[train], (Y[train]))\n",
    "        prediction = model.predict(X[test])\n",
    "        scores.append(haversine_50thP_95thP_mean(prediction[:,0], prediction[:,1], Y[test][:,0], Y[test][:,1]))\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready for a simple regression model. </br>\n",
    "First we try to train a regression model based only on baseline lat,lng values!</br>\n",
    "This is because those values are relativlely aligned with the ground values. From it we gain 2 things:\n",
    "* Understand if this is a good direction. getting a lousy score here (over tens of meters) will mean something wrong - sort of an hello world.\n",
    "* Obtain a benchmark - a minimal required performance for other regression models, hyper parameters and features we'll test. As we add up more information and sophisticated models we expect better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "\n",
    "\n",
    "predict_columns = ['latDeg_grnd', 'lngDeg_grnd']\n",
    "basic_baseline_columns = ['latDeg_bsln', 'lngDeg_bsln']\n",
    "\n",
    "lr_df = df[predict_columns + basic_baseline_columns]\n",
    "model = LR()\n",
    "score = utilize_model(model=model, data=lr_df)\n",
    "print(f'Prediction accuracy mean is: {score} meters using the {type(model).__name__} model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the entire data set. We want to improve this score by adding some other features.<br>\n",
    "Simply trying to train based on **all** of the data will **fail** as there are NaN values! (in most columns of the data, actually)</br>\n",
    "Investigating shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def columns_with_na_count(df):\n",
    "    return df.isna().any()[lambda i: i].shape[0]\n",
    "\n",
    "print(f'There are {columns_with_na_count(df)} columns containing at least 1 NaN value!')\n",
    "print(f'There are {columns_with_na_count(df.T)} rows containing at least 1 NaN value!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relativly low rate of empty values containing rows - less then 1%. For now what we can do is discard them and train on what's left. Later we'll try to make up for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_na_df = df.dropna()\n",
    "model = LR(normalize=True)\n",
    "score = utilize_model(model, data=no_na_df)\n",
    "print(f'Prediction accuracy mean is: {score} meters using the {type(model).__name__} model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression models we are going to try are:\n",
    "* Linear\n",
    "* Lasso\n",
    "* Ridge\n",
    "* ElasticNet\n",
    "* SGDRegressor\n",
    "\n",
    "The logic behind choosing all of those is essentially the same - matching continous values to continous target values.<br>\n",
    "Starting out with linear regression we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = utilize_model(model, data=no_na_df)\n",
    "print(f'Prediction accuracy mean is: {score} meters using the {type(model).__name__} model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to lasso and ridge. Those models are useful in the same sense that the linear regression one is (and indeed the linear model is a special case of those models).<br>\n",
    "They differ by the fact that they use a bias to the slope of the relations they find between the regressors and the predict values. The values that give the lowest variance (distance scoring method in this case) are faovred. Those hyperparameters (alpha value and the l1_ratio for elastic net) are optimized by trial-and-error:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "alpha_settings = np.concatenate([np.arange(.1,1,.1), np.arange(1,10,1), np.arange(10,100,5)])\n",
    "lasso_scores = []\n",
    "ridge_scores = []\n",
    "for alpha in alpha_settings:\n",
    "    lasso_scores.append(utilize_model(Lasso(alpha=alpha, tol=1, max_iter=500), no_na_df))\n",
    "    ridge_scores.append(utilize_model(Ridge(alpha=alpha, tol=1, max_iter=500), no_na_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores obtaind can be plotted to get a sense of the influence of the alpha value on the models performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_alpha_data = pd.DataFrame({'alpha_value': alpha_settings, 'lasso_score': lasso_scores, 'ridge_score': ridge_scores})\n",
    "sns.lineplot(data=[model_alpha_data['lasso_score'], model_alpha_data['ridge_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet, as a combination of both the previous methods, requires two parameters as hyper parameters. here we also iterate over a range of values for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet as EN\n",
    "\n",
    "\n",
    "en_scores = []\n",
    "for alpha in alpha_settings[:1]:\n",
    "    for ratio in np.arange(.2, 1, .2):\n",
    "        en_scores.append((alpha, ratio, utilize_model(EN(alpha=alpha, l1_ratio=ratio), no_na_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here plotting the hyper parameters gives out a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_scores_df = pd.DataFrame(en_scores, columns=['alpha','l1_ratio','score']).round(5)\n",
    "en_scores_df = en_scores_df.pivot('alpha','l1_ratio','score')\n",
    "sns.heatmap(en_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD regressor is a linear model fitted by minimizing a loss using Stochastic Gradient Descent.</br>\n",
    "the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).</br>\n",
    "We're going to use the default loss function: \"squared_loss\".</br>\n",
    "Since this model expects a label of a single dimension, we try to predict the lat and long values seperatly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "Y = no_na_df[predict_columns]\n",
    "#X = mean_fill_df.drop(predict_columns, axis=1)\n",
    "X = no_na_df[['latDeg_bsln', 'lngDeg_bsln']]\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_normed = scaler_x.fit_transform(X)\n",
    "y_normed = scaler_y.fit_transform(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_normed, y_normed, random_state=0, test_size=0.2)\n",
    "\n",
    "y_train_lat = y_train[:,0]\n",
    "y_train_lng = y_train[:,1]\n",
    "\n",
    "model = SGDRegressor(max_iter=5000, loss=\"squared_loss\")\n",
    "model.fit(x_train, y_train_lat)\n",
    "lat_prediction = model.predict(x_test)\n",
    "\n",
    "model = SGDRegressor(max_iter=5000, loss=\"squared_loss\")\n",
    "model.fit(x_train, y_train_lng)\n",
    "lng_prediction = model.predict(x_test)\n",
    "\n",
    "prediction = np.vstack((lat_prediction, lng_prediction)).T\n",
    "y_arr = y_test\n",
    "\n",
    "x_inv = scaler_x.inverse_transform(prediction)\n",
    "y_inv = scaler_y.inverse_transform(y_arr)\n",
    "\n",
    "distance_score = haversine_50thP_95thP_mean(x_inv[:,0], x_inv[:,1], y_inv[:,0], y_inv[:,1])\n",
    "print(f'Prediction accuracy: {distance_score} meters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and address the missing values.</br>\n",
    "A obviously better approach regarding missing values would be to make up for the loss. One (very) naive method is filling the gaps with 0's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_fill_df = df.fillna(0)\n",
    "print(f'There are {columns_with_na_count(zero_fill_df)} columns containing at least 1 NaN value!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for splitting the data, fitting and predicting.<br>\n",
    "Using a linear regression model and the embbeded normalization we get a disappointing result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LR(normalize=True)\n",
    "score = utilize_model(model, data=zero_fill_df)\n",
    "print(f'Prediction accuracy mean is: {score} meters using the {type(model).__name__} model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was expected though. Filling empty values with 0's doesn't make much sense as the values being filled are physical properties of satelittes, some with relation to the cell phone.<br>\n",
    "0s for those values can throw the model way off. We can try a different method for filling missing data that makes more sense.<br>\n",
    "One way to start is to fill empty values by the mean value of their respective columns (over all data, which still is far from ideal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_fill_df = df.fillna(df.mean())\n",
    "\n",
    "utilize_model(LR(normalize=True), data=mean_fill_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimistic as this approach for missing values may be, the prediction is still very far off. <br>\n",
    "While trying to think of better ways (and implementations) for handeling this problem it occoured to us that using the pivotted data may not be the best idea for regression. <br>\n",
    "Using the derived data in an unpivotted manner may be useful here. The interesting values will still be those that describe continous, physcial properties of the satellites but the categorial, discrete types (like the pivot axis - satelite ID) will be dropped. <br>\n",
    "The reasoning for this decision is that it could be helpful to only concentrate on the phycisal values first (like a location of **a** satelite and its distance from a cell phone) rather than trying to address more complex relationships that this type of model might not be able to describe naturally.<br><br>\n",
    "Simply put - forget about **which** satellite it was, let's try to predict based on **where** it was.<br><br>\n",
    "We'll start by matching the baseline and derived sets with and the ground set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_key = ['collectionName', 'phoneName', 'millisSinceGpsEpoch']\n",
    "\n",
    "bsln_grnd = pd.merge(bsln_trn, ground[sample_key + ['latDeg', 'lngDeg']], how='inner', on=sample_key, suffixes=['', '_grnd'])\n",
    "\n",
    "continous_features = ['correctedPrM', 'xSatPosM', 'ySatPosM', 'zSatPosM', 'xSatVelMps', 'ySatVelMps', 'zSatVelMps', 'satClkBiasM', 'satClkDriftMps']\n",
    "derived_reg = derived[continous_features + sample_key + ['svid']]\n",
    "\n",
    "# group derived by spesific satellite sample\n",
    "drvd_reg_grp = derived_reg.groupby(sample_key + ['svid']).agg('first')  \n",
    "# match with the baselinet and ground sets\n",
    "drvd_bsln_grnd = pd.merge_asof(drvd_reg_grp.sort_values('millisSinceGpsEpoch'), bsln_grnd.sort_values('millisSinceGpsEpoch'), on=['millisSinceGpsEpoch'], tolerance=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then again, we're cleaning it from null values and non numbers - dropping for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drvd_bsln_grnd_clean = drvd_bsln_grnd.dropna()\n",
    "unpivot_data = drvd_bsln_grnd_clean.select_dtypes(include=['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to apply the linear regression model on the new data set gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilize_model(LR(normalize=True), filter_on_train_data=(unpivot_data), data=unpivot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The use of neural network seems natural to the task due to the fact that we have multiple datapoints<br>\n",
    "and no specific idea regarding which are the important features.<br>\n",
    "feature engineering is exhasting task so we will let the neural network do it for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following architecture will be a vanilla Neural Network.\n",
    "The model consists of 1 input layer with 22 inputs (number of features)<br>\n",
    "first hidden layer consists of 22 neurons with tanh activation function (used tanh for continous -1 to 1 results)<br>\n",
    "two hidden layers consists of 44 neurons with relu activation to avoid the vanishing gradient problem.<br>\n",
    "last hidden layer consists of 22 neurons with relu activation function.<br>\n",
    "output layer consists of 2 output neurons each represent the latitude and longitude of the location of the vehicle.<br>\n",
    "The model will use ADAM optimizer with decaying learning rate function starting with 0.0001.<br>\n",
    "The loss function we will use in this architecture would be the evaluation metric that had accompanied us during the whole project,<br>\n",
    "the mean between the 95th precentile and the median of the haversine distances between the ground truth and the predicted values.<br>\n",
    "the following illustrates the architecture using random weight initialization.<br>\n",
    "the edges opacity is propotional to the edges weights.<br>\n",
    "\n",
    "\n",
    "![alt text](nn.svg \"neural network archetecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function\n",
    "Our NN model would be implemented using `tensorflow` library.<br>\n",
    "Because the evaluation metric of the competition is the haversine distance loss, <br>\n",
    "A good NN implementation would reduce this following metric.<br>\n",
    "Therefore we will build a loss function exactly for that.<br>\n",
    "`Keras` library make sure to take into account the gradients calculations using the given loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def haversine_loss(y_true, y_pred):\n",
    "    PI_ON_180 = tf.constant(np.pi / 180, dtype=tf.float32)\n",
    "    RADIUS_M = tf.constant(6_377_000, dtype = tf.float32)\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "\n",
    "    yt_rad = y_true * PI_ON_180\n",
    "    yp_rad = y_pred * PI_ON_180\n",
    "\n",
    "    delta = yt_rad - yp_rad\n",
    "    v = delta / 2\n",
    "    v = tf.sin(v)\n",
    "    v = v**2\n",
    "\n",
    "    a = v[:,1] + tf.cos(yt_rad[:,1]) * tf.cos(yp_rad[:,1]) * v[:,0] \n",
    "    c = tf.sqrt(a)\n",
    "    c = 2* tf.math.asin(c)\n",
    "    c = c*RADIUS_M\n",
    "    \n",
    "    p50 = tfp.stats.percentile(c, 50)\n",
    "    p95 = tfp.stats.percentile(c, 95)\n",
    "    \n",
    "    final = tf.reduce_mean(tf.convert_to_tensor([p50, p95]))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSimpleNN(hp):\n",
    "    in_nodes = x_train_norm.shape[1]\n",
    "    inputLayer = Input(in_nodes)\n",
    "    \n",
    "    layer = tf.keras.layers.Dense(name=\"lat_layer_1\",hp.Int(\"lat_layer_1\", in_nodes//3, in_nodes*2, in_nodes//3))(inputLayer)\n",
    "    if hp.Boolean(\"leaky_layer_1\"):\n",
    "        activation = LeakyReLU()\n",
    "    else:\n",
    "        activation = ReLU()\n",
    "    layer = activation(layer)\n",
    "    for i in range(hp.Int(\"n_layers\", 2, 20)):\n",
    "        layer = tf.keras.layers.Dense(hp.Int(f\"layer_{i}\", in_nodes//3, in_nodes*2, in_nodes//6))(layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}\"):\n",
    "            activation = LeakyReLU()\n",
    "        else:\n",
    "            activation = ReLU()\n",
    "        layer = activation(layer)\n",
    "    outputs = Dense(2)(layer)\n",
    "    model = tf.keras.Model(inputLayer, outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hp.Float('learning rate', 0.0000001, 0.001))\n",
    "    model.compile(loss=haversine_loss,\n",
    "                optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latitude / Langitude correction NN\n",
    "The following model would still be a vanilla neural network but some of the connections are custom made.<br>\n",
    "We created a network that will stand along our intution regarding how we think the model should operate.<br>\n",
    "Our main idea behind this network is to create an error correction model,<br>\n",
    "We already have rough results and all we need to do is refine them.<br>\n",
    "Therefore it seems logical to start with model that for each latitude and langitude parameters predicts it's<br>\n",
    "baseline prediction and add for each of them an error correction factor that is generated via a neural network we will call<br>\n",
    "lat/lng addative unit. finally we will calculate the loss function using the haversine_loss function and use Adam as optimizer.<br>\n",
    "In order to predict the baseline predictions and diviate from them we will initialize all the weights to 0 and then only for the pass through units<br>\n",
    "add a 1 weight for latitude and langitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(hp):\n",
    "    # Get the number of features\n",
    "    in_nodes = x_train_norm.shape[1]\n",
    "    inputLayer = Input(in_nodes)\n",
    "    # Set seperate the latitude feature and langitude feature to two different layers that will be passed directly\n",
    "    # to the output after correction given from the mid NN\n",
    "    latIn = Lambda(lambda x: x[:, 0:1], output_shape=((1,)))(inputLayer)\n",
    "    lngIn = Lambda(lambda x: x[:, 1:2], output_shape=((1,)))(inputLayer)\n",
    "    \n",
    "    # Latitude addative (correction) unit\n",
    "    # We will choose around 1 third to 2 times the number of features due to simple rule of thumb\n",
    "    lat_layer = Dense(name=\"lat_layer_1\" ,units=hp.Int(\"lat_layer_1\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(inputLayer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lat\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lat\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lat\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "    for i in range(2, hp.Int(\"n_layers_lat\", 2, 10)):\n",
    "        lat_layer = tf.keras.layers.Dense(name=f\"lat_layer_{i}\", units=hp.Int(f\"lat_layer_{i}\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lat\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lat\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lat\")\n",
    "        lat_layer = activation(lat_layer)\n",
    "        \n",
    "    # Langitude addative (correction) unit\n",
    "    # the same as in the latitude unit logic\n",
    "    lng_layer = tf.keras.layers.Dense(name=\"lng_layer_1\", units=hp.Int(\"lng_layer_1\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(inputLayer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lng\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lng\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lng\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "    for i in range(2, hp.Int(\"n_layers_lng\", 2, 10)):\n",
    "        lng_layer = tf.keras.layers.Dense(name=f\"lng_layer_{i}\", units=hp.Int(f\"lng_layer_{i}\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lng\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lng\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lng\")\n",
    "        lng_layer = activation(lng_layer)\n",
    "    \n",
    "    lat_add_unit = Dense(name=\"lat_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "    lng_add_unit = Dense(name=\"lng_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "    \n",
    "    # Correct the latitude value by the value outputed from the middle neural network\n",
    "    latIn_plus_add = Concatenate(name=\"concate_lat_and_add\")([latIn, lat_add_unit])\n",
    "    output_lat = Dense(name=\"lat_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(latIn_plus_add)\n",
    "    \n",
    "    # Correct the langitude value by the value outputed from the middle neural network\n",
    "    lngIn_plus_add = Concatenate(name=\"concate_lng_and_add\")([lngIn, lng_add_unit])\n",
    "    output_lng = Dense(name=\"lng_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lngIn_plus_add)\n",
    "    \n",
    "    # Concat the lat and lng together and output them as a pair\n",
    "    outputs = Concatenate(name=\"output\")([output_lat, output_lng])\n",
    "    \n",
    "    model = tf.keras.Model(inputLayer, outputs)\n",
    "    \n",
    "    # Set the weights that the initial prediction will simply predict the given lat/lng from baseline\n",
    "    weights = model.get_weights()\n",
    "    weights[-2] = np.array([[1],[0]])\n",
    "    weights[-4] = np.array([[1],[0]])\n",
    "    model.set_weights(weights)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hp.Float('learning rate', 0.0000001, 0.001))\n",
    "    model.compile(loss=haversine_loss,\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "#tf.keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Static model with no hyperparameters\n",
    "The following is the same NN architecture as before but with static hyperparameters<br>\n",
    "We use it only to validate that our bypassing connections for lat/lng are working well even before<br>\n",
    "the learning proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelStatic():\n",
    "    in_nodes = x_train_norm.shape[1]\n",
    "    inputLayer = Input(in_nodes)\n",
    "    latIn = Lambda(lambda x: x[:, 0:1], output_shape=((1,)))(inputLayer)\n",
    "    lngIn = Lambda(lambda x: x[:, 1:2], output_shape=((1,)))(inputLayer)\n",
    "    \n",
    "    # Latitude addative unit\n",
    "    lat_layer = Dense(name=\"lat_layer_1\" ,units=1, kernel_initializer='zeros', bias_initializer='zeros')(inputLayer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lat\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lat\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "    for i in range(2, 2):\n",
    "        lat_layer = tf.keras.layers.Dense(name=f\"lat_layer_{i}\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "        if False:\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lat\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lat\")\n",
    "        lat_layer = activation(lat_layer)\n",
    "        \n",
    "    # Langitude addative unit\n",
    "    lng_layer = tf.keras.layers.Dense(name=\"lng_layer_1\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(inputLayer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lng\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lng\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "    for i in range(2, 2):\n",
    "        lng_layer = tf.keras.layers.Dense(name=f\"lng_layer_{i}\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "        if False:\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lng\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lng\")\n",
    "        lng_layer = activation(lng_layer)\n",
    "    \n",
    "    lat_add_unit = Dense(name=\"lat_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "    lng_add_unit = Dense(name=\"lng_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "    \n",
    "    latIn_plus_add = Concatenate(name=\"concate_lat_and_add\")([latIn, lat_add_unit])\n",
    "    output_lat = Dense(name=\"lat_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(latIn_plus_add)\n",
    "    \n",
    "    lngIn_plus_add = Concatenate(name=\"concate_lng_and_add\")([lngIn, lng_add_unit])\n",
    "    output_lng = Dense(name=\"lng_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lngIn_plus_add)\n",
    "    \n",
    "    outputs = Concatenate(name=\"output\")([output_lat, output_lng])\n",
    "    \n",
    "    model = tf.keras.Model(inputLayer, outputs)\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    weights[-2] = np.array([[1],[0]])\n",
    "    weights[-4] = np.array([[1],[0]])\n",
    "    model.set_weights(weights)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.00097200077)\n",
    "    model.compile(loss=haversine_loss,\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "#tf.keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make a single feedthrough our NN.<br>\n",
    "As excpected due to the network architecture when initialized, it should predict exactly the latitude and langitude of the input data.<br>\n",
    "Because we havn't yet trained the model the weights are as initialized which means that all is 0 except the ones \"passing\" the lat/lng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModelStatic()\n",
    "loss_train = tf.keras.backend.eval(haversine_loss(np.array(y_train), model.predict(np.array(x_train))))\n",
    "loss_val = tf.keras.backend.eval(haversine_loss(np.array(y_val), model.predict(np.array(x_val))))\n",
    "loss_train_norm = tf.keras.backend.eval(haversine_loss(y_train_norm, model.predict(x_train_norm)))\n",
    "loss_val_norm = tf.keras.backend.eval(haversine_loss(y_val_norm, model.predict(x_val_norm)))\n",
    "print(f\"loss_train: {loss_train} | loss_val: {loss_val} | loss_train_norm: {loss_train_norm} | loss_val_norm: {loss_val_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vizualization above is an illustration of the neural network desribed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have general idea for a model how do we know how many layers in each \"lat/lon unit\" to use?<br>\n",
    "How many nodes at each one of these layers? Which activation function to use? What is the best learning rate?<br>\n",
    "Most of the data scientists tell you a rule of thumb and mostly trial and error.<br>\n",
    "there is no simple equation for setting these parameters.<br>\n",
    "We can automate the process of hyperparameter tuning using very useful package called keras-tuner.<br>\n",
    "using keras tuner we sample random parameters for our model at predefined places, the tuner will automate this procedure<br>\n",
    "by a set of randomized tests on several sampled models. Finally it will return us a model containing the best models sampled.<br><br>\n",
    "We chose to let the tuner choose the following parameters in our model:<br>\n",
    "* no. of layers in the NN that output a factor in which to add/subtract to the latitude max 8 layers\n",
    "* no. of layers in the NN that output a factor in which to add/subtract to the langitude max 8 layers\n",
    "* no. of nodes at each hidden layer\n",
    "* Adam learning rate, ranges between 0.0000001 - 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNER_DIR = 'tuner_logs_complex_nn_5'\n",
    "tuner = kt.RandomSearch(buildModel, objective='val_loss', max_trials=150, directory=TUNER_DIR)\n",
    "tuner.search(x=x_train_norm, y=y_train_norm, epochs=20, validation_data=(x_val_norm, y_val_norm), verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 5 hours to find the best hyperparameters.<br>\n",
    "We don't want all this work to go in vain, Quickly save the best model before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.save('best_params_model_3_8.hdf5')\n",
    "model = tf.keras.models.load_model('best_params_model_3_8.hdf5', custom_objects={'haversine_loss': haversine_loss})\n",
    "print(f\"model learning rate is: {tf.keras.backend.eval(model.optimizer.lr)}\")\n",
    "plot_model(model, to_file='best_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossTrainValAll(train, labels):\n",
    "    x = scaler_x_trn.transform(train)\n",
    "    y = scaler_y_trn.transform(labels)\n",
    "    pred = model.predict(x)\n",
    "    pred_i = scaler_y_trn.inverse_transform(pred)\n",
    "    return haversine_50thP_95thP_mean(np.array(labels)[:,0], np.array(labels)[:,1], pred_i[:,0],  pred_i[:,1])\n",
    "\n",
    "print(f\"train loss: {lossTrainValAll(x_train, y_train)} | validation loss: {lossTrainValAll(x_val, y_val)} | Overall loss: {lossTrainValAll(train, labels)}\")\n",
    "print(f\"Baseline loss: {evaluate(train, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is actually slightly worse than the basic baseline predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot our model that had been described before is illustrated.\n",
    "The keras tuner package after 150 different random hyperparameters combinations in the given hyperparamers space which each was trained for 20 epochs<br>\n",
    "the best model had been found. The best model resulted in an Overall loss: 5.3 after 20 epochs.<br>\n",
    "It used 6 layers in the latitude correction unit, 2 layers in the langitude layer, <br>\n",
    "learning rate of 2.358e-05 and varied combinations of ReLU/LeakyReLU activations and no. of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning process\n",
    "We will continue to train the best model from the keras tuner to get to our desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly it is important to time the learning process in order to know the performance of our model.<br>\n",
    "`stop_early` - we will configure callback functions to stop early the learning procedure if for 50 epochs we hadn't improved<br>\n",
    "our model according to the loss function for shorter training process.<br>\n",
    "`mcp_save` - a callback function that will save the best performing model.<br>\n",
    "each epoch it would evaluate the model and if it's better the previous ones it would save it instead\n",
    "We had set to train our model using 10000 epochs.<br>\n",
    "<br>\n",
    "`learningRateDecayCallback` - a callback class which controls the learning rate parameter of the model<br>\n",
    "after each epoch end the function is called, the function will decay the learning rate using the following formula:<br>\n",
    "`learning_rate_next = learning_rate_current * 1 / ( 1 + decay*epoch_number )` whereas `decay` is the decaying rate.<br>\n",
    "we had set the `decay` factor to `1/(number_of_epochs*100)` (times 100 for less drastical decrease in the learning rate)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1.3585707822348922e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 1000\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='max')\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)\n",
    "class learningRateDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, decay):\n",
    "        self.decay = decay\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.optimizer.lr =  tf.keras.backend.eval(self.model.optimizer.lr) * (1/(1+self.decay*epoch))\n",
    "        print(\"learning rate: {}\".format(tf.keras.backend.eval(self.model.optimizer.lr)))\n",
    "        \n",
    "\n",
    "learningRateDecay = learningRateDecayCallback(1/(epochs*100))\n",
    "history = model.fit(x_train_norm, y_train_norm, epochs=epochs, validation_data=(x_val_norm, y_val_norm), callbacks=[mcp_save, tb_callback, tb_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the best performing epoch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\".mdl_wts.hdf5\", compile=False)\n",
    "reconstructed_model.compile(loss=haversine_loss,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=best_model.optimizer.lr))\n",
    "model = reconstructed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve.<br>\n",
    "in the horizontal axis the epoch number and in the vertical axis the loss metric.<br>\n",
    "we will use log scale in this learning curve because everything simply will be dwarfed compared to the first epochs<br>\n",
    "even though the loss continue to decrease. <br>\n",
    "We want to see a negative decline on the curve and finally alignment along the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "plt.plot(list(range(len(history.history['loss']))), history.history['loss'], label='train_loss', c='r')\n",
    "plt.plot(list(range(len(history.history['loss']))), history.history['val_loss'], alpha=0.5, label='val_loss', c='b', ls='--')\n",
    "plt.ylabel('haversine loss')\n",
    "plt.xlabel('ephoch')\n",
    "plt.title('Learning curve')\n",
    "plt.legend()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation:\n",
    "Although the model was fed with baseline input data it actually managed to perform worst than the raw baseline inputs.<br>\n",
    "If we zoom into the learning curve we can see that the curve was negativly inclined, therefore we more epochs I believe the model could converge to at least the baseline performance.<br>\n",
    "In my opinion although the baseline lat lon predictions are nice in order to significally make a change we must incorprate much more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM timeseries approach\n",
    "Our data is timeseries data. therefore a more natural approach would be to use an algorithm which is known for working well with time series data.<br>\n",
    "the RNN architectures are known as such.What is RNN model?<br>\n",
    "RNN stands for recurrent neural network, RNN is simply a neural network with memory from the previous passes-through.<br>\n",
    "that means that in a given window of time which we predefine each pass through.<br>\n",
    "RNNs are using a concept called squential memory, a mechanism that make it easier to replicate sequences.<br>\n",
    "RNN layers has an recurrent loop which a hidden state from the previous steps is fed forward to the current hidden layer. <br>\n",
    "finally after passing through all the samples in sequence our hidden layer would have information from all the previous samples.<br>\n",
    "this curse of action can be refered as using temporal memory. Now we can pass it through a simple feed forward network.<br>\n",
    "the problem with RNN is that due to the fact that it is essentially extremly deep neural network and each gradient is calculated with respect to the layer before it as it backpropagates our gradients will exponentialy decrease and eventually be so close to zero that the first layers won't learn at all. ( the vanishing gradient problem), <br>\n",
    "luckly a solution to the problem is available. LSTM neural network.<br>\n",
    "LSTM stands for long short memory neural network, the LSTM cells are comprised of gates that regulate the flow of <br>\n",
    "which data to keep or to discard, and by the use of gates it can learn to use only relavent information to make predictions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.models.Sequential([\n",
    "    LSTM(4, input_shape=trainX.shape[1:], return_sequences=True, activation='relu'),\n",
    "    LSTM(2, return_sequences=False, activation='relu'),\n",
    "    Dense(2)\n",
    "])\n",
    "\n",
    "lstm.compile(optimizer='adam', loss=haversine_loss_lstm)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "in_nodes = len(train.columns)\n",
    "inputLayer = Input(trainX.shape[1:])\n",
    "latIn = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(inputLayer)\n",
    "lngIn = Lambda(lambda x: x[:, -1, 1:2], output_shape=((1,)))(inputLayer)\n",
    "lat_layer = LSTM((2*in_nodes)//3, name=\"lat_layer_1_lstm\", kernel_initializer='zeros', bias_initializer='zeros', return_sequences=True)(inputLayer)\n",
    "if True:\n",
    "    activation = LeakyReLU(name=\"leaky_layer_1_lat_lstm\")\n",
    "else:\n",
    "    activation = ReLU(name=\"relu_layer_1_lat_lstm\")\n",
    "lat_layer = activation(lat_layer)\n",
    "lat_layer = Dropout(0.2, name='lat_1_dropout_lstm')(lat_layer)\n",
    "lat_layers =  4\n",
    "for i in range(2,lat_layers):\n",
    "    return_sequences = (i+1 != lat_layers)\n",
    "    lat_layer = tf.keras.layers.LSTM(in_nodes//3,name=f\"lat_layer_{i}_lstm\", kernel_initializer='zeros',\\\n",
    "                                      bias_initializer='zeros', return_sequences=return_sequences)(lat_layer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=f\"leaky_layer_{i}_lat_lstm\")\n",
    "    else:\n",
    "        activation = ReLU(name=f\"relu_layer_{i}_lat_lstm\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "    lat_layer = Dropout(0.2, name=f'lat_{i}_dropout_lstm')(lat_layer)\n",
    "\n",
    "# Fully connected part\n",
    "###########\n",
    "# Latitude addative unit\n",
    "# Latitude addative unit\n",
    "lat_layer = Dense(name=\"lat_layer_1\" ,units=(2*in_nodes)//3, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "if False:\n",
    "    activation = LeakyReLU(name=\"leaky_layer_1_lat\")\n",
    "else:\n",
    "    activation = ReLU(name=\"relu_layer_1_lat\")\n",
    "lat_layer = activation(lat_layer)\n",
    "for i in range(2, 4):\n",
    "    lat_layer = tf.keras.layers.Dense(name=f\"lat_layer_{i}\", units=in_nodes//3, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=f\"leaky_layer_{i}_lat\")\n",
    "    else:\n",
    "        activation = ReLU(name=f\"relu_layer_{i}_lat\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "\n",
    "# Add to the baseline original prediction\n",
    "lat_add_unit = Dense(name=\"lat_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "#lat_add_unit = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(lat_add_unit)\n",
    "latIn_plus_add = Concatenate(name=\"concate_lat_and_add\")([latIn, lat_add_unit])\n",
    "output_lat = Dense(name=\"lat_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(latIn_plus_add)\n",
    "\n",
    "lng_layer = LSTM((2*in_nodes)//3, name=\"lng_layer_1_lstm\", kernel_initializer='zeros', bias_initializer='zeros', return_sequences=True)(inputLayer)\n",
    "if True:\n",
    "    activation = LeakyReLU(name=\"leaky_layer_1_lng_lstm\")\n",
    "else:\n",
    "    activation = ReLU(name=\"relu_layer_1_lng_lstm\")\n",
    "lng_layer = activation(lng_layer)\n",
    "lng_layer = Dropout(0.2, name='lng_1_dropout_lstm')(lng_layer)\n",
    "lng_layers =  4\n",
    "for i in range(2,lng_layers):\n",
    "    return_sequences = (i+1 != lng_layers)\n",
    "    lng_layer = tf.keras.layers.LSTM(in_nodes//3,name=f\"lng_layer_{i}_lstm\", kernel_initializer='zeros',\\\n",
    "                                      bias_initializer='zeros', return_sequences=return_sequences)(lng_layer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=f\"leaky_layer_{i}_lng_lstm\")\n",
    "    else:\n",
    "        activation = ReLU(name=f\"relu_layer_{i}_lng_lstm\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "    lng_layer = Dropout(0.2, name=f'lng_{i}_dropout_lstm')(lng_layer)\n",
    "\n",
    "# Fully connected part\n",
    "###########\n",
    "# lngitude addative unit\n",
    "# lngitude addative unit\n",
    "lng_layer = Dense(name=\"lng_layer_1\" ,units=(2*in_nodes)//3, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "if False:\n",
    "    activation = LeakyReLU(name=\"leaky_layer_1_lng\")\n",
    "else:\n",
    "    activation = ReLU(name=\"relu_layer_1_lng\")\n",
    "lng_layer = activation(lng_layer)\n",
    "for i in range(2, 4):\n",
    "    lng_layer = tf.keras.layers.Dense(name=f\"lng_layer_{i}\", units=in_nodes//3, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "    if False:\n",
    "        activation = LeakyReLU(name=f\"leaky_layer_{i}_lng\")\n",
    "    else:\n",
    "        activation = ReLU(name=f\"relu_layer_{i}_lng\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "\n",
    "# Add to the baseline original prediction\n",
    "lng_add_unit = Dense(name=\"lng_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "#lng_add_unit = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(lng_add_unit)\n",
    "lngIn_plus_add = Concatenate(name=\"concate_lng_and_add\")([lngIn, lng_add_unit])\n",
    "output_lng = Dense(name=\"lng_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lngIn_plus_add)\n",
    "outputs = Concatenate(name=\"output\")([output_lat, output_lng])\n",
    "ls = tf.keras.Model(inputLayer, outputs)\n",
    "\n",
    "weights = ls.get_weights()\n",
    "weights[-2] = np.array([[1],[0]])\n",
    "weights[-4] = np.array([[1],[0]])\n",
    "ls.set_weights(weights)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.3585707822348922e-05)\n",
    "ls.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ls.fit(x=trainX, y=trainY, epochs=240, validation_data=(valX, valY), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelLSTM(hp):\n",
    "    in_nodes = len(train.columns)\n",
    "    inputLayer = Input(trainX.shape[1:])\n",
    "    latIn = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(inputLayer)\n",
    "    lngIn = Lambda(lambda x: x[:, -1, 1:2], output_shape=((1,)))(inputLayer)\n",
    "    \n",
    "    # Latitude correction unit\n",
    "    # LSTM part\n",
    "    ###########\n",
    "    lat_layer = LSTM(hp.Int(\"lat_layer_1_lstm\", in_nodes//3, in_nodes*2, in_nodes//3), name=\"lat_layer_1_lstm\", kernel_initializer='zeros', bias_initializer='zeros', return_sequences=True)(inputLayer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lat_lstm\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lat_lstm\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lat_lstm\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "    lat_layer = Dropout(hp.Float('lat_1_dropout_lstm',0.1,0.25), name='lat_1_dropout_lstm')(lat_layer)\n",
    "    lat_layers =  hp.Int(\"n_layers_lat_lstm\", 2, 5)\n",
    "    for i in range(2,lat_layers):\n",
    "        return_sequences = i+1 != lat_layers\n",
    "        lat_layer = tf.keras.layers.LSTM(hp.Int(f\"lat_layer_{i}_lstm\", in_nodes//3, in_nodes*2, in_nodes//3),name=f\"lat_layer_{i}_lstm\", kernel_initializer='zeros',\\\n",
    "                                          bias_initializer='zeros', return_sequences=return_sequences)(lat_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lat_lstm\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lat_lstm\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lat_lstm\")\n",
    "        lat_layer = activation(lat_layer)\n",
    "        lat_layer = Dropout(hp.Float(f'lat_{i}_dropout_lstm',0.1,0.25), name=f'lat_{i}_dropout_lstm')(lat_layer)\n",
    "    \n",
    "    # Fully connected part\n",
    "    ###########\n",
    "    # Latitude addative unit\n",
    "    lat_layer = Dense(name=\"lat_layer_1\" ,units=hp.Int(\"lat_layer_1\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lat\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lat\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lat\")\n",
    "    lat_layer = activation(lat_layer)\n",
    "    for i in range(2, hp.Int(\"n_layers_lat\", 2, 4)):\n",
    "        lat_layer = tf.keras.layers.Dense(name=f\"lat_layer_{i}\", units=hp.Int(f\"lat_layer_{i}\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lat\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lat\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lat\")\n",
    "        lat_layer = activation(lat_layer)\n",
    "    \n",
    "    # Add to the baseline original prediction\n",
    "    lat_add_unit = Dense(name=\"lat_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lat_layer)\n",
    "    #lat_add_unit = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(lat_add_unit)\n",
    "    latIn_plus_add = Concatenate(name=\"concate_lat_and_add\")([latIn, lat_add_unit])\n",
    "    output_lat = Dense(name=\"lat_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(latIn_plus_add)\n",
    "    \n",
    "    #######################################################################\n",
    "    # lngitude correction unit\n",
    "    # LSTM part\n",
    "    ###########\n",
    "    lng_layer = LSTM(hp.Int(\"lng_layer_1_lstm\", in_nodes//3, in_nodes*2, in_nodes//3), name=\"lng_layer_1_lstm\", kernel_initializer='zeros', bias_initializer='zeros', return_sequences=True)(inputLayer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lng_lstm\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lng_lstm\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lng_lstm\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "    lng_layer = Dropout(hp.Float('lng_1_dropout_lstm',0.1,0.25), name='lng_1_dropout_lstm')(lng_layer)\n",
    "    lng_layers =  hp.Int(\"n_layers_lng_lstm\", 2, 5)\n",
    "    for i in range(2,lng_layers):\n",
    "        return_sequences = i+1 != lng_layers\n",
    "        lng_layer = tf.keras.layers.LSTM(hp.Int(f\"lng_layer_{i}_lstm\", in_nodes//3, in_nodes*2, in_nodes//3),name=f\"lng_layer_{i}_lstm\", kernel_initializer='zeros',\\\n",
    "                                          bias_initializer='zeros', return_sequences=return_sequences)(lng_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lng_lstm\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lng_lstm\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lng_lstm\")\n",
    "        lng_layer = activation(lng_layer)\n",
    "        lng_layer = Dropout(hp.Float(f'lng_{i}_dropout_lstm',0.1,0.25), name=f'lng_{i}_dropout_lstm')(lng_layer)\n",
    "    \n",
    "    # Fully connected part\n",
    "    ###########\n",
    "    # lngitude addative unit\n",
    "    lng_layer = Dense(name=\"lng_layer_1\" ,units=hp.Int(\"lng_layer_1\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "    if hp.Boolean(\"leaky_layer_1_lng\"):\n",
    "        activation = LeakyReLU(name=\"leaky_layer_1_lng\")\n",
    "    else:\n",
    "        activation = ReLU(name=\"relu_layer_1_lng\")\n",
    "    lng_layer = activation(lng_layer)\n",
    "    for i in range(2, hp.Int(\"n_layers_lng\", 2, 4)):\n",
    "        lng_layer = tf.keras.layers.Dense(name=f\"lng_layer_{i}\", units=hp.Int(f\"lng_layer_{i}\", in_nodes//3, in_nodes*2, in_nodes//3), kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "        if hp.Boolean(f\"leaky_layer_{i}_lng\"):\n",
    "            activation = LeakyReLU(name=f\"leaky_layer_{i}_lng\")\n",
    "        else:\n",
    "            activation = ReLU(name=f\"relu_layer_{i}_lng\")\n",
    "        lng_layer = activation(lng_layer)\n",
    "    \n",
    "    # Add to the baseline original prediction\n",
    "    lng_add_unit = Dense(name=\"lng_addative_nn_out\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lng_layer)\n",
    "    #lng_add_unit = Lambda(lambda x: x[:, -1, 0:1], output_shape=((1,)))(lng_add_unit)\n",
    "    lngIn_plus_add = Concatenate(name=\"concate_lng_and_add\")([lngIn, lng_add_unit])\n",
    "    output_lng = Dense(name=\"lng_plus_correction\", units=1, kernel_initializer='zeros', bias_initializer='zeros')(lngIn_plus_add)\n",
    "    ###########################################################\n",
    "    outputs = Concatenate(name=\"output\")([output_lat, output_lng])\n",
    "    \n",
    "    model = tf.keras.Model(inputLayer, outputs)\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    weights[-2] = np.array([[1],[0]])\n",
    "    weights[-4] = np.array([[1],[0]])\n",
    "    model.set_weights(weights)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hp.Float('learning rate', 0.0000001, 0.001))\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "#tf.keras.utils.vis_utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNER_DIR = 'tuner_logs_lstm'\n",
    "tuner = kt.RandomSearch(buildModelLSTM, objective='val_loss', max_trials=10, directory=TUNER_DIR)\n",
    "tuner.search(x=trainX, y=trainY, epochs=5, validation_data=(valX, valY), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_loss_lstm(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, [-1, 2])\n",
    "    PI_ON_180 = tf.constant(np.pi / 180, dtype=tf.float32)\n",
    "    RADIUS_M = tf.constant(6_377_000, dtype = tf.float32)\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "\n",
    "    yt_rad = y_true * PI_ON_180\n",
    "    yp_rad = y_pred * PI_ON_180\n",
    "\n",
    "    delta = yt_rad - yp_rad\n",
    "    v = delta / 2\n",
    "    v = tf.sin(v)\n",
    "    v = v**2\n",
    "\n",
    "    a = v[:,1] + tf.cos(yt_rad[:,1]) * tf.cos(yp_rad[:,1]) * v[:,0] \n",
    "    c = tf.sqrt(a)\n",
    "    c = 2* tf.math.asin(c)\n",
    "    c = c*RADIUS_M\n",
    "    \n",
    "    p50 = tfp.stats.percentile(c, 50)\n",
    "    p95 = tfp.stats.percentile(c, 95)\n",
    "    \n",
    "    final = tf.reduce_mean(tf.convert_to_tensor([p50, p95]))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm.fit(trainX, trainY, epochs=100, batch_size=30, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00097200077)\n",
    "lstm.compile(loss=haversine_loss, optimizer=optimizer)\n",
    "history = lstm.fit(trainX, trainY, epochs=5, batch_size=16, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ls.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_i = scaler_y_trn_clc.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haversine_50thP_95thP_mean(np.array(y_train_clc.iloc[n_past:]['latDeg']), np.array(y_train_clc.iloc[n_past:]['lngDeg']), pred_i[:,0], pred_i[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsln_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestions for the future\n",
    "\n",
    "- ##### LSTM Neural network<br>\n",
    "To improve the model we would try and use a LSTM network instead of simple one.<br>\n",
    "The LSTM can handle well the nature of time series using deep architecture.<br>\n",
    "I think LSTM can even handle it better than RNN due to the RNN weakness of vanishing gradinent problem over deeper layers.<br>\n",
    "Therefore for each sample we would have a memory of our route, which can help a lot diminishing errors<br>\n",
    "- ##### Use derived dataset<br>\n",
    "Incorprate the derived dataset measurements per satelite with the baseline dataset<br>\n",
    "In order to archive higher level of complexity in our calculation new data must be introduced regarding various parameters such as<br>\n",
    "distance from satelite to device, the location of the satelite and much more<br>\n",
    "- ##### Incorprate historic measurements from similar lat/lon pairs\n",
    "Try and make use of historic measurements from other phones on current phone.\n",
    "Take advantage using other measurements in the same time from other collections\n",
    "- ##### Collect more data using the GNSS Android API\n",
    "Maybe all we need is to introduce some variance to the model<br>\n",
    "By collecting data here in tel aviv we are generelizing our model to work outside san fransisco bay area and less prone to overfitting\n",
    "- ##### Incorprate Accelometer & Gyroscope measurements to the dataset\n",
    "By using features originating from other phone sensors we can roughly predict how the car should move from the first point to the last point<br>\n",
    "Therefore using the kalman filter and introducing the data to the neural network as well we can predict the loss in our baseline prediction and fix it<br>\n",
    "This should be a very promising step, the data is written in raw log measurements therefore it is not easy task to approach neccessaraly\n",
    "- ##### Hyperparameter tuning\n",
    "Using keras tuner to determine the best hyperparameters for our model\n",
    "- ##### Outlier detection\n",
    "Detect outlier measurements and decide on cleaning policy, whether dropping the sample or changing its value to more appropriate one.\n",
    "- ##### Run the notebook on GPU for faster training\n",
    "The training process took 3 days for simple 3 layer model with low learning rate but still it is slow, <br>\n",
    "We would try to run our taining on google colab notebook with GPU in order for faster feedback loop on our model performance.<br>\n",
    "- ##### Use a better filler value for the training pivot table for the regression model\n",
    "Using '0' to fill satelite properties for satelite IDs that samples miss is a bad idea, throwing off the model.\n",
    "- ##### Calculate some more evaluation metrics in order to evaluate better the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
